name: LlamaFarm Default Prompts Configuration
version: 1.0.0
description: Default configuration with comprehensive prompt templates and strategies
enabled: true
default_strategy: context_aware_strategy
global_prompts:
- global_id: system_context
  name: System Context
  description: Provides general system context for all prompts
  system_prompt: You are a helpful AI assistant integrated with LlamaFarm, a comprehensive
    document processing and RAG system. You have access to retrieved documents and
    should provide accurate, helpful responses based on the available information.
  prefix_prompt: null
  suffix_prompt: null
  applies_to:
  - '*'
  excludes: []
  conditions: {}
  priority: 10
  enabled: true
  tags: []
  created_by: null
- global_id: quality_guidelines
  name: Quality Guidelines
  description: Ensures high-quality responses
  system_prompt: null
  prefix_prompt: Please provide a clear, accurate, and helpful response. If you're
    uncertain about something, acknowledge the uncertainty.
  suffix_prompt: null
  applies_to:
  - '*'
  excludes:
  - debug_*
  - test_*
  conditions: {}
  priority: 20
  enabled: true
  tags: []
  created_by: null
- global_id: domain_medical
  name: Medical Domain Context
  description: Medical domain expertise context
  system_prompt: You are analyzing medical documents and should be precise, cite sources
    when available, and note any limitations in the provided information. Always recommend
    consulting healthcare professionals for medical decisions.
  prefix_prompt: null
  suffix_prompt: null
  applies_to:
  - medical_*
  excludes: []
  conditions:
    domain: medical
  priority: 30
  enabled: true
  tags: []
  created_by: null
templates:
  summarization:
    template_id: summarization
    name: Document Summarization
    type: basic
    template: 'Please summarize the following document(s):


      {{ context | format_documents }}


      Provide a concise summary highlighting the key points:


      Summary:'
    input_variables:
    - context
    optional_variables: []
    metadata:
      use_case: summarization
      complexity: low
      domain: general
      tags:
      - summary
      - document
      - overview
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.350757'
      updated_at: '2025-07-31 10:06:57.350762'
      version: 1.0.0
      description: General document summarization template
      examples:
      - description: Summarizing a research article
        input:
          context:
          - title: AI Research Paper
            content: This paper presents a novel approach to neural network optimization
              using gradient descent variations. The proposed method shows 15% improvement
              in training speed and 8% better accuracy on benchmark datasets.
        expected_output: The research paper introduces a new neural network optimization
          method that improves training speed by 15% and accuracy by 8% compared to
          standard approaches.
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  qa_detailed:
    template_id: qa_detailed
    name: Detailed Question Answering
    type: basic
    template: 'Context Information:

      {{ context | format_documents(max_length=2000) }}


      Question: {{ query }}


      Please provide a comprehensive answer based on the context above. If the context
      doesn''t contain sufficient information to answer the question completely, please
      indicate what information is missing.


      Answer:'
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: detailed_qa
      complexity: medium
      domain: general
      tags:
      - qa
      - detailed
      - comprehensive
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.350929'
      updated_at: '2025-07-31 10:06:57.350930'
      version: 1.0.0
      description: Detailed question answering with comprehensive responses
      examples:
      - description: Complex question requiring detailed analysis
        input:
          query: How does machine learning impact modern healthcare?
          context:
          - title: ML in Healthcare
            content: Machine learning applications in healthcare include diagnostic
              imaging, drug discovery, personalized treatment plans, and predictive
              analytics for patient outcomes.
        expected_output: Based on the context, machine learning significantly impacts
          modern healthcare through multiple applications...
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 1
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  qa_basic:
    template_id: qa_basic
    name: Basic Question Answering
    type: basic
    template: 'Based on the following context:


      {{ context | format_documents }}


      Question: {{ query }}


      Answer:'
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: general_qa
      complexity: low
      domain: general
      tags:
      - qa
      - basic
      - general
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.351053'
      updated_at: '2025-07-31 10:06:57.351053'
      version: 1.0.0
      description: Simple question answering template for general queries
      examples:
      - description: Basic factual question
        input:
          query: What is machine learning?
          context:
          - title: ML Introduction
            content: Machine learning is a subset of AI that enables computers to
              learn from data.
        expected_output: Based on the context, machine learning is a subset of AI
          that enables computers to learn from data.
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 1
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  chat_assistant:
    template_id: chat_assistant
    name: Chat Assistant
    type: chat
    template: '{% if context %}Available Information:

      {{ context | format_documents(max_length=1000) }}


      {% endif %}User: {{ query }}


      Assistant: I''ll help you with that. {% if context %}Based on the information
      available, {% endif %}'
    input_variables:
    - query
    optional_variables:
    - context
    metadata:
      use_case: conversational
      complexity: low
      domain: general
      tags:
      - chat
      - assistant
      - conversation
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.351468'
      updated_at: '2025-07-31 10:06:57.351471'
      version: 1.0.0
      description: Conversational assistant template
      examples:
      - description: Basic chat interaction
        input:
          query: Hello! Can you help me understand what machine learning is?
          context:
          - title: ML Basics
            content: Machine learning is a method of data analysis that automates
              analytical model building.
        expected_output: 'User: Hello! Can you help me understand what machine learning
          is?


          Assistant: I''ll help you with that. Based on the information available,
          machine learning is a method of data analysis that automates analytical
          model building...'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 1
        max_length: 1000
        required: true
      context:
        type: list
        required: false
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  few_shot_classification:
    template_id: few_shot_classification
    name: Few-Shot Classification
    type: few_shot
    template: 'Classify the following text based on these examples:


      Examples:

      {{ examples }}


      Text to classify: {{ query }}


      Category:'
    input_variables:
    - examples
    - query
    optional_variables: []
    metadata:
      use_case: classification
      complexity: medium
      domain: general
      tags:
      - classification
      - few-shot
      - examples
      - categorization
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.351856'
      updated_at: '2025-07-31 10:06:57.351857'
      version: 1.0.0
      description: Few-shot learning classification template
      examples:
      - description: Email classification
        input:
          query: Meeting scheduled for next Tuesday at 2 PM
          examples: '‚Ä¢ ''Please review the attached document'' ‚Üí Business

            ‚Ä¢ ''Happy birthday! Hope you have a great day!'' ‚Üí Personal

            ‚Ä¢ ''Your order has been shipped'' ‚Üí Notification'
        expected_output: Business
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 1
        max_length: 2000
        required: true
      examples:
        type: str
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  ab_testing:
    template_id: ab_testing
    name: A/B Testing Comparison
    type: advanced
    template: '# A/B Testing Analysis


      ## Test Overview

      {{ test_description }}


      ## Variant A Details

      **Name**: {{ variant_a_name }}

      **Configuration**: {{ variant_a_config }}

      **Sample Responses**:

      {{ variant_a_responses }}


      ## Variant B Details

      **Name**: {{ variant_b_name }}

      **Configuration**: {{ variant_b_config }}

      **Sample Responses**:

      {{ variant_b_responses }}


      ## Evaluation Metrics

      {{ evaluation_metrics }}


      üß™ **A/B TESTING COMPARATIVE ANALYSIS**


      üìä **Performance Comparison**


      **Response Quality Metrics**

      ‚Ä¢ **Accuracy**: Which variant provides more accurate information?

      ‚Ä¢ **Completeness**: Which variant gives more comprehensive answers?

      ‚Ä¢ **Relevance**: Which variant better addresses the user queries?

      ‚Ä¢ **Consistency**: Which variant shows more consistent performance?


      **User Experience Metrics**

      ‚Ä¢ **Clarity**: Which variant communicates more clearly?

      ‚Ä¢ **Usefulness**: Which variant provides more actionable insights?

      ‚Ä¢ **Engagement**: Which variant better engages users?

      ‚Ä¢ **Satisfaction**: Which variant would users prefer?


      **Technical Performance**

      ‚Ä¢ **Response Time**: How do processing speeds compare?

      ‚Ä¢ **Resource Usage**: What are the computational requirements?

      ‚Ä¢ **Reliability**: Which variant fails less often?

      ‚Ä¢ **Scalability**: Which variant handles load better?


      ‚öñÔ∏è **Side-by-Side Evaluation**


      **Variant A Strengths:**

      ‚Ä¢ [List specific advantages of Variant A]

      ‚Ä¢ [Examples of superior performance]

      ‚Ä¢ [Unique capabilities or features]


      **Variant A Weaknesses:**

      ‚Ä¢ [List specific limitations of Variant A]

      ‚Ä¢ [Examples of inferior performance]

      ‚Ä¢ [Missing features or capabilities]


      **Variant B Strengths:**

      ‚Ä¢ [List specific advantages of Variant B]

      ‚Ä¢ [Examples of superior performance]

      ‚Ä¢ [Unique capabilities or features]


      **Variant B Weaknesses:**

      ‚Ä¢ [List specific limitations of Variant B]

      ‚Ä¢ [Examples of inferior performance]

      ‚Ä¢ [Missing features or capabilities]


      üìà **Quantitative Comparison**


      **Scoring Matrix** (1-10 scale):


      | Metric | Variant A | Variant B | Winner |

      |--------|-----------|-----------|--------|

      | Accuracy | [Score] | [Score] | [A/B/Tie] |

      | Completeness | [Score] | [Score] | [A/B/Tie] |

      | Relevance | [Score] | [Score] | [A/B/Tie] |

      | Clarity | [Score] | [Score] | [A/B/Tie] |

      | Usefulness | [Score] | [Score] | [A/B/Tie] |

      | Consistency | [Score] | [Score] | [A/B/Tie] |

      | Engagement | [Score] | [Score] | [A/B/Tie] |

      | Performance | [Score] | [Score] | [A/B/Tie] |


      **Total Scores:**

      ‚Ä¢ Variant A: [Total]/80 ([Percentage]%)

      ‚Ä¢ Variant B: [Total]/80 ([Percentage]%)


      üèÜ **Test Results & Recommendation**


      **Statistical Significance:**

      ‚Ä¢ Sample size: [Number of test cases]

      ‚Ä¢ Confidence level: [Percentage]%

      ‚Ä¢ Effect size: [Small/Medium/Large]


      **Winner**: [Variant A / Variant B / No Clear Winner]

      **Confidence**: [High/Medium/Low]


      **Key Findings:**

      ‚Ä¢ [Most important discovery from the test]

      ‚Ä¢ [Surprising or unexpected results]

      ‚Ä¢ [Patterns observed across test cases]


      **Contextual Considerations:**

      ‚Ä¢ **Use Case Fit**: Which variant is better for specific scenarios?

      ‚Ä¢ **User Segments**: Do different user types prefer different variants?

      ‚Ä¢ **Trade-offs**: What are the key trade-offs between variants?

      ‚Ä¢ **Edge Cases**: How do variants handle unusual or challenging inputs?


      üí° **Strategic Recommendations**


      **Immediate Actions:**

      ‚Ä¢ [What should be implemented right away]


      **Long-term Strategy:**

      ‚Ä¢ [How to evolve based on these findings]


      **Future Testing:**

      ‚Ä¢ [What should be tested next]

      ‚Ä¢ [Additional variants to consider]

      ‚Ä¢ [Metrics to add or refine]


      **Implementation Plan:**

      ‚Ä¢ [Steps to roll out the winning variant]

      ‚Ä¢ [Monitoring and measurement plan]

      ‚Ä¢ [Rollback strategy if needed]


      üîç **Additional Insights**


      **Unexpected Behaviors:**

      ‚Ä¢ [Any surprising findings or edge cases]


      **User Feedback Themes:**

      ‚Ä¢ [Common patterns in user responses]


      **Technical Observations:**

      ‚Ä¢ [Performance or implementation insights]


      **Bias Considerations:**

      ‚Ä¢ [Potential biases in the test setup or evaluation]


      **Next Steps:**

      ‚Ä¢ [Specific actions to take based on results]'
    input_variables:
    - test_description
    - variant_a_name
    - variant_a_config
    - variant_a_responses
    - variant_b_name
    - variant_b_config
    - variant_b_responses
    - evaluation_metrics
    optional_variables: []
    metadata:
      use_case: ab_testing
      complexity: high
      domain: evaluation
      tags:
      - ab_testing
      - comparison
      - evaluation
      - optimization
      - statistics
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.352329'
      updated_at: '2025-07-31 10:06:57.352330'
      version: 1.0.0
      description: Comprehensive A/B testing comparison template for evaluating different
        AI configurations
      examples:
      - description: Comparing two prompt strategies
        input:
          test_description: Testing chain-of-thought vs direct answer prompts for
            math problems
          variant_a_name: Chain of Thought
          variant_a_config: Prompts include step-by-step reasoning instructions
          variant_a_responses: 'Response 1: Shows detailed work, correct answer

            Response 2: Clear steps, correct answer

            Response 3: Verbose but accurate'
          variant_b_name: Direct Answer
          variant_b_config: Prompts ask for immediate answers without showing work
          variant_b_responses: 'Response 1: Quick answer, correct

            Response 2: Fast response, minor error

            Response 3: Concise, correct'
          evaluation_metrics: Accuracy, response time, user preference for explanation
            detail
        expected_output: '**Winner**: Chain of Thought

          **Key Finding**: Higher accuracy (95% vs 87%) despite longer response time

          **Recommendation**: Use Chain of Thought for complex problems, Direct Answer
          for simple queries'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      test_description:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      variant_a_name:
        type: str
        min_length: 1
        max_length: 100
        required: true
      variant_a_config:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      variant_a_responses:
        type: str
        min_length: 20
        max_length: 5000
        required: true
      variant_b_name:
        type: str
        min_length: 1
        max_length: 100
        required: true
      variant_b_config:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      variant_b_responses:
        type: str
        min_length: 20
        max_length: 5000
        required: true
      evaluation_metrics:
        type: str
        min_length: 10
        max_length: 1000
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  chain_of_thought:
    template_id: chain_of_thought
    name: Chain of Thought Reasoning
    type: advanced
    template: "Context: {{ context | format_documents }}\n\nQuestion: {{ query }}\n\
      \nLet me work through this step by step:\n\n1Ô∏è‚É£ **Understanding the Question**\n\
      What exactly is being asked and what kind of analysis is needed?\n\n2Ô∏è‚É£ **Key\
      \ Information from Context**  \nWhat relevant facts and data can I extract from\
      \ the provided context?\n\n3Ô∏è‚É£ **Step-by-Step Reasoning**\nHow do these pieces\
      \ of information connect to answer the question?\n\n4Ô∏è‚É£ **Final Conclusion**\n\
      Based on my analysis, here is my reasoned answer:\n\n**Answer:**"
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: analytical_reasoning
      complexity: high
      domain: general
      tags:
      - reasoning
      - analysis
      - step-by-step
      - complex
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.352449'
      updated_at: '2025-07-31 10:06:57.352449'
      version: 1.0.0
      description: Chain of thought reasoning for complex questions
      examples:
      - description: Complex analytical question
        input:
          query: Why might renewable energy adoption vary between countries?
          context:
          - title: Energy Policy
            content: Countries have different energy policies, economic conditions,
              natural resources, and technological capabilities that influence renewable
              energy adoption.
        expected_output: 'Let me work through this step by step: 1. Understanding
          the question: We need to analyze factors affecting renewable energy adoption
          across different countries...'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  comparative_analysis:
    template_id: comparative_analysis
    name: Comparative Analysis
    type: advanced
    template: "Documents to compare:\n{{ context | format_documents }}\n\nAnalysis\
      \ request: {{ query }}\n\n\U0001F4CA **COMPARATIVE ANALYSIS**\n\n\U0001F91D\
      \ **Similarities**\n‚Ä¢ What common themes, approaches, or characteristics do\
      \ these items share?\n‚Ä¢ Where do they align or agree?\n\n‚ö° **Key Differences**\
      \  \n‚Ä¢ What are the main contrasts and distinctions?\n‚Ä¢ How do their approaches,\
      \ outcomes, or features differ?\n\n\U0001F50D **Analysis & Insights**\n‚Ä¢ What\
      \ patterns emerge from this comparison?\n‚Ä¢ What are the implications of these\
      \ similarities and differences?\n\n**Conclusion:**"
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: comparative_analysis
      complexity: high
      domain: general
      tags:
      - comparison
      - analysis
      - contrast
      - insights
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.352576'
      updated_at: '2025-07-31 10:06:57.352576'
      version: 1.0.0
      description: Compare and contrast multiple documents or concepts
      examples:
      - description: Technology comparison
        input:
          query: Compare the advantages and disadvantages of cloud vs on-premises
            infrastructure
          context:
          - title: Cloud Computing
            content: Cloud computing offers scalability, cost-effectiveness, and automatic
              updates, but may have security concerns and dependency on internet connectivity.
          - title: On-Premises Infrastructure
            content: On-premises infrastructure provides complete control and security
              but requires significant upfront investment and ongoing maintenance.
        expected_output: '## Similarities:

          Both cloud and on-premises solutions provide computing infrastructure for
          business needs...


          ## Differences:

          Cloud offers scalability and lower upfront costs, while on-premises provides
          more control...'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  prompt_evaluation:
    template_id: prompt_evaluation
    name: Prompt Effectiveness Evaluation
    type: domain_specific
    template: '# Prompt Effectiveness Evaluation


      ## Prompt Being Evaluated

      ```

      {{ prompt_template }}

      ```


      ## Test Cases and Responses

      {{ test_cases }}


      ## Evaluation Context

      {{ evaluation_context }}


      üìù **PROMPT EVALUATION FRAMEWORK**


      üéØ **Clarity & Instructions**

      ‚Ä¢ **Instruction Clarity**: Are the instructions clear and unambiguous?

      ‚Ä¢ **Task Definition**: Is the desired task/output clearly defined?

      ‚Ä¢ **Format Specification**: Are output format requirements clearly stated?

      ‚Ä¢ **Role Definition**: Is the AI''s role or persona clearly established?


      üß† **Cognitive Load & Complexity**

      ‚Ä¢ **Cognitive Demand**: How much reasoning does the prompt require?

      ‚Ä¢ **Context Length**: Is the prompt appropriately concise yet comprehensive?

      ‚Ä¢ **Step Breakdown**: Are complex tasks broken into manageable steps?

      ‚Ä¢ **Mental Model**: Does the prompt help establish the right mental framework?


      ‚ö° **Performance Effectiveness**

      ‚Ä¢ **Consistency**: Does the prompt produce consistent outputs across similar
      inputs?

      ‚Ä¢ **Accuracy**: How accurate are the responses generated by this prompt?

      ‚Ä¢ **Completeness**: Do responses fully address the intended requirements?

      ‚Ä¢ **Reliability**: How reliably does the prompt work across different scenarios?


      üé® **Design Quality**

      ‚Ä¢ **Structure**: Is the prompt well-organized and logically structured?

      ‚Ä¢ **Examples**: Are examples provided where helpful?

      ‚Ä¢ **Constraints**: Are appropriate constraints and boundaries set?

      ‚Ä¢ **Flexibility**: Can the prompt handle variations in input?


      üìä **Evaluation Scores**


      **Clarity Metrics:**

      ‚Ä¢ Instruction Clarity: [Score 1-5] - [Justification]

      ‚Ä¢ Task Definition: [Score 1-5] - [Justification]

      ‚Ä¢ Format Specification: [Score 1-5] - [Justification]

      ‚Ä¢ Role Definition: [Score 1-5] - [Justification]


      **Effectiveness Metrics:**

      ‚Ä¢ Consistency: [Score 1-5] - [Justification]

      ‚Ä¢ Accuracy: [Score 1-5] - [Justification]

      ‚Ä¢ Completeness: [Score 1-5] - [Justification]

      ‚Ä¢ Reliability: [Score 1-5] - [Justification]


      **Design Metrics:**

      ‚Ä¢ Structure: [Score 1-5] - [Justification]

      ‚Ä¢ Examples Usage: [Score 1-5] - [Justification]

      ‚Ä¢ Constraint Setting: [Score 1-5] - [Justification]

      ‚Ä¢ Flexibility: [Score 1-5] - [Justification]


      **Overall Prompt Score**: [Total]/60

      **Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) /
      Very Poor (0-19)]


      üîç **Detailed Analysis**


      **What Works Well:**

      ‚Ä¢ [Specific strengths of the prompt]


      **Areas for Improvement:**

      ‚Ä¢ [Specific weaknesses and issues]


      **Common Response Patterns:**

      ‚Ä¢ [Patterns observed in AI responses]


      **Edge Cases & Failure Modes:**

      ‚Ä¢ [Scenarios where the prompt fails or underperforms]


      üí° **Optimization Recommendations**


      **High Priority:**

      ‚Ä¢ [Most critical improvements needed]


      **Medium Priority:**

      ‚Ä¢ [Secondary improvements that would help]


      **Optimization Techniques:**

      ‚Ä¢ [Specific prompt engineering techniques to apply]


      **Revised Prompt Suggestion:**

      ```

      [Provide an improved version of the prompt if significant changes are recommended]

      ```'
    input_variables:
    - prompt_template
    - test_cases
    - evaluation_context
    optional_variables: []
    metadata:
      use_case: prompt_optimization
      complexity: high
      domain: evaluation
      tags:
      - prompt_engineering
      - evaluation
      - optimization
      - testing
      - effectiveness
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.352993'
      updated_at: '2025-07-31 10:06:57.352997'
      version: 1.0.0
      description: Comprehensive evaluation template for prompt effectiveness and
        optimization
      examples:
      - description: Evaluating a summarization prompt
        input:
          prompt_template: 'Summarize the following text in 3 sentences: {{ text }}'
          test_cases: 'Test 1: Long research paper -> Generated a 2-sentence summary
            missing key points

            Test 2: News article -> Generated a 4-sentence summary with good coverage

            Test 3: Technical documentation -> Generated unclear summary with jargon'
          evaluation_context: Testing for general document summarization across different
            text types
        expected_output: '**Overall Prompt Score**: 35/60 - Fair

          **Key Issues**: Inconsistent length adherence, lacks domain adaptation

          **Recommendations**: Add length constraints, specify clarity requirements,
          include examples'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      prompt_template:
        type: str
        min_length: 10
        max_length: 5000
        required: true
      test_cases:
        type: str
        min_length: 20
        max_length: 10000
        required: true
      evaluation_context:
        type: str
        min_length: 10
        max_length: 1000
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  response_scoring:
    template_id: response_scoring
    name: Response Quality Scoring
    type: domain_specific
    template: "# Response Quality Scoring\n\n## Original Query\n{{ query }}\n\n##\
      \ AI Response to Score\n{{ response }}\n\n## Reference Materials (if available)\n\
      {{ reference_materials | format_documents }}\n\n## Scoring Criteria\n{{ scoring_criteria\
      \ }}\n\n\U0001F3C6 **RESPONSE QUALITY SCORING**\n\n\U0001F4CB **Core Quality\
      \ Dimensions**\n\n**1. Content Quality**\n‚Ä¢ **Accuracy**: Is the information\
      \ factually correct and reliable?\n‚Ä¢ **Relevance**: Does the response directly\
      \ address the query?\n‚Ä¢ **Depth**: How thorough and comprehensive is the response?\n\
      ‚Ä¢ **Currency**: Is the information up-to-date and current?\n\n**2. Communication\
      \ Quality** \n‚Ä¢ **Clarity**: Is the response clear and easy to understand?\n\
      ‚Ä¢ **Structure**: Is the response well-organized and logical?\n‚Ä¢ **Conciseness**:\
      \ Is the response appropriately concise without losing important information?\n\
      ‚Ä¢ **Tone**: Is the tone appropriate for the context and audience?\n\n**3. Technical\
      \ Quality**\n‚Ä¢ **Coherence**: Does the response flow logically from point to\
      \ point?\n‚Ä¢ **Completeness**: Are all aspects of the query addressed?\n‚Ä¢ **Citations**:\
      \ Are sources properly referenced when applicable?\n‚Ä¢ **Formatting**: Is the\
      \ response well-formatted and readable?\n\n**4. Utility & Actionability**\n\
      ‚Ä¢ **Usefulness**: How helpful is the response to the user?\n‚Ä¢ **Actionability**:\
      \ Does the response provide actionable insights or next steps?\n‚Ä¢ **Practical\
      \ Value**: Can the user apply this information effectively?\n‚Ä¢ **Follow-up Guidance**:\
      \ Does it help the user understand next steps?\n\n\U0001F4CA **Detailed Scoring\
      \ Matrix**\n\n**Content Quality Scores:**\n‚Ä¢ Accuracy: [Score 1-10] - [Specific\
      \ reasoning]\n‚Ä¢ Relevance: [Score 1-10] - [Specific reasoning]\n‚Ä¢ Depth: [Score\
      \ 1-10] - [Specific reasoning]\n‚Ä¢ Currency: [Score 1-10] - [Specific reasoning]\n\
      \n**Communication Quality Scores:**\n‚Ä¢ Clarity: [Score 1-10] - [Specific reasoning]\n\
      ‚Ä¢ Structure: [Score 1-10] - [Specific reasoning]\n‚Ä¢ Conciseness: [Score 1-10]\
      \ - [Specific reasoning]\n‚Ä¢ Tone: [Score 1-10] - [Specific reasoning]\n\n**Technical\
      \ Quality Scores:**\n‚Ä¢ Coherence: [Score 1-10] - [Specific reasoning]\n‚Ä¢ Completeness:\
      \ [Score 1-10] - [Specific reasoning]\n‚Ä¢ Citations: [Score 1-10] - [Specific\
      \ reasoning]\n‚Ä¢ Formatting: [Score 1-10] - [Specific reasoning]\n\n**Utility\
      \ & Actionability Scores:**\n‚Ä¢ Usefulness: [Score 1-10] - [Specific reasoning]\n\
      ‚Ä¢ Actionability: [Score 1-10] - [Specific reasoning]\n‚Ä¢ Practical Value: [Score\
      \ 1-10] - [Specific reasoning]\n‚Ä¢ Follow-up Guidance: [Score 1-10] - [Specific\
      \ reasoning]\n\n\U0001F4C8 **Summary Scores**\n‚Ä¢ **Content Quality**: [Total]/40\
      \ ([Percentage]%)\n‚Ä¢ **Communication Quality**: [Total]/40 ([Percentage]%)\n\
      ‚Ä¢ **Technical Quality**: [Total]/40 ([Percentage]%)\n‚Ä¢ **Utility & Actionability**:\
      \ [Total]/40 ([Percentage]%)\n\n**OVERALL RESPONSE SCORE**: [Total]/160 ([Percentage]%)\n\
      \n**Quality Grade**: \n‚Ä¢ 90-100%: Exceptional ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚Ä¢ 80-89%: Excellent ‚≠ê‚≠ê‚≠ê‚≠ê\n\
      ‚Ä¢ 70-79%: Good ‚≠ê‚≠ê‚≠ê\n‚Ä¢ 60-69%: Fair ‚≠ê‚≠ê\n‚Ä¢ Below 60%: Needs Improvement ‚≠ê\n\n\U0001F4A1\
      \ **Qualitative Assessment**\n\n**Key Strengths:**\n‚Ä¢ [What the response does\
      \ exceptionally well]\n\n**Areas for Improvement:**\n‚Ä¢ [Specific areas that\
      \ need enhancement]\n\n**Missing Elements:**\n‚Ä¢ [Important information or aspects\
      \ not addressed]\n\n**Standout Features:**\n‚Ä¢ [Particularly impressive or noteworthy\
      \ aspects]\n\n\U0001F3AF **Improvement Recommendations**\n\n**High Priority:**\n\
      ‚Ä¢ [Most critical improvements needed]\n\n**Medium Priority:**\n‚Ä¢ [Secondary\
      \ improvements that would enhance quality]\n\n**Enhancement Suggestions:**\n\
      ‚Ä¢ [Specific ways to make the response even better]\n\n**Comparative Analysis:**\n\
      ‚Ä¢ How does this response compare to typical responses for similar queries?\n\
      ‚Ä¢ What would make this response exemplary?"
    input_variables:
    - query
    - response
    - scoring_criteria
    optional_variables:
    - reference_materials
    metadata:
      use_case: response_evaluation
      complexity: medium
      domain: evaluation
      tags:
      - scoring
      - quality_assessment
      - response_evaluation
      - metrics
      - benchmarking
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.353124'
      updated_at: '2025-07-31 10:06:57.353124'
      version: 1.0.0
      description: Comprehensive scoring template for AI response quality assessment
      examples:
      - description: Scoring a technical explanation response
        input:
          query: How does machine learning work?
          response: Machine learning is a type of AI where computers learn patterns
            from data without being explicitly programmed. It works by training algorithms
            on large datasets to recognize patterns and make predictions.
          scoring_criteria: Technical accuracy, clarity for general audience, completeness
            of explanation
          reference_materials:
          - title: ML Fundamentals
            content: Machine learning involves training algorithms on data to make
              predictions or decisions without explicit programming.
        expected_output: '**OVERALL RESPONSE SCORE**: 112/160 (70%)

          **Quality Grade**: Good ‚≠ê‚≠ê‚≠ê

          **Key Strengths**: Clear explanation, accurate basics

          **Areas for Improvement**: Could include examples, more depth on algorithm
          types'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 5
        max_length: 1000
        required: true
      response:
        type: str
        min_length: 10
        max_length: 10000
        required: true
      scoring_criteria:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      reference_materials:
        type: list
        required: false
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  rag_evaluation:
    template_id: rag_evaluation
    name: RAG System Evaluation
    type: domain_specific
    template: '# RAG System Evaluation


      ## Query

      {{ query }}


      ## Retrieved Documents

      {{ retrieved_docs | format_documents }}


      ## Generated Response

      {{ generated_response }}


      ## Ground Truth (if available)

      {{ ground_truth }}


      üîç **RAG SYSTEM EVALUATION**


      üìö **Retrieval Quality Assessment**

      ‚Ä¢ **Relevance**: Are the retrieved documents relevant to the query?

      ‚Ä¢ **Coverage**: Do the documents contain information needed to answer the query?

      ‚Ä¢ **Diversity**: Do the documents provide diverse perspectives or complementary
      information?

      ‚Ä¢ **Ranking**: Are the most relevant documents ranked highest?


      üéØ **Answer Quality Assessment**

      ‚Ä¢ **Accuracy**: Is the generated response factually correct?

      ‚Ä¢ **Completeness**: Does the response fully address the query?

      ‚Ä¢ **Coherence**: Is the response well-structured and coherent?

      ‚Ä¢ **Citation**: Does the response appropriately reference the source documents?


      üîó **Retrieval-Generation Alignment**

      ‚Ä¢ **Grounding**: Is the response properly grounded in the retrieved documents?

      ‚Ä¢ **Hallucination Check**: Does the response contain information not present
      in the retrieved docs?

      ‚Ä¢ **Consistency**: Is the response consistent with the retrieved information?

      ‚Ä¢ **Source Attribution**: Are claims properly attributed to sources?


      üìä **Scoring Matrix**


      **Retrieval Metrics:**

      ‚Ä¢ Relevance: [Score 1-5] - [Justification]

      ‚Ä¢ Coverage: [Score 1-5] - [Justification]

      ‚Ä¢ Diversity: [Score 1-5] - [Justification]

      ‚Ä¢ Ranking Quality: [Score 1-5] - [Justification]


      **Generation Metrics:**

      ‚Ä¢ Accuracy: [Score 1-5] - [Justification]

      ‚Ä¢ Completeness: [Score 1-5] - [Justification]

      ‚Ä¢ Coherence: [Score 1-5] - [Justification]

      ‚Ä¢ Citation Quality: [Score 1-5] - [Justification]


      **Alignment Metrics:**

      ‚Ä¢ Grounding: [Score 1-5] - [Justification]

      ‚Ä¢ Hallucination: [Score 1-5] - [Lower = more hallucination]

      ‚Ä¢ Consistency: [Score 1-5] - [Justification]


      **Overall RAG Score**: [Total]/55

      **Grade**: [Excellent/Good/Fair/Poor]


      ‚ö†Ô∏è **Key Issues Identified**

      ‚Ä¢ [List major problems or concerns]


      ‚úÖ **Strengths**

      ‚Ä¢ [List what worked well]


      üîß **Recommendations**

      ‚Ä¢ [Specific suggestions for improvement]'
    input_variables:
    - query
    - retrieved_docs
    - generated_response
    optional_variables:
    - ground_truth
    metadata:
      use_case: rag_evaluation
      complexity: high
      domain: evaluation
      tags:
      - rag
      - evaluation
      - retrieval
      - generation
      - quality_assessment
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.353313'
      updated_at: '2025-07-31 10:06:57.353314'
      version: 1.0.0
      description: Comprehensive evaluation template for RAG system performance
      examples:
      - description: Evaluating RAG response quality
        input:
          query: What are the benefits of renewable energy?
          retrieved_docs:
          - title: Solar Energy Benefits
            content: Solar energy reduces carbon emissions and provides long-term
              cost savings.
          - title: Wind Power Advantages
            content: Wind power is sustainable and creates jobs in rural communities.
          generated_response: Renewable energy offers multiple benefits including
            reduced carbon emissions from solar power, long-term cost savings, job
            creation in rural areas through wind power, and overall sustainability.
          ground_truth: Benefits include environmental protection, economic advantages,
            and energy independence.
        expected_output: '**Retrieval Quality**: 4/5 - Documents are relevant and
          provide good coverage

          **Generation Quality**: 4/5 - Response is accurate and well-grounded

          **Overall RAG Score**: 44/55'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 5
        max_length: 1000
        required: true
      retrieved_docs:
        type: list
        required: true
      generated_response:
        type: str
        min_length: 10
        max_length: 5000
        required: true
      ground_truth:
        type: str
        required: false
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  code_analysis:
    template_id: code_analysis
    name: Code Analysis
    type: domain_specific
    template: 'Code to analyze:

      ```

      {{ context }}

      ```


      Analysis request: {{ query }}


      üíª **CODE ANALYSIS REPORT**


      üèóÔ∏è **Structure & Organization**

      ‚Ä¢ How is the code structured and organized?

      ‚Ä¢ Is it readable and well-formatted?

      ‚Ä¢ Are naming conventions consistent?


      ‚öôÔ∏è **Logic & Functionality**

      ‚Ä¢ Does the code accomplish its intended purpose?

      ‚Ä¢ Is the logic clear and efficient?

      ‚Ä¢ Are there any logical errors or edge cases missed?


      ‚ö†Ô∏è **Potential Issues**

      ‚Ä¢ Are there any bugs, security vulnerabilities, or performance problems?

      ‚Ä¢ What improvements could be made?

      ‚Ä¢ Are there any code smells or anti-patterns?


      üìã **Recommendations**

      ‚Ä¢ What specific changes would improve this code?

      ‚Ä¢ Are there better approaches or design patterns to consider?

      ‚Ä¢ What would make this code more maintainable?


      üìä **Overall Assessment**

      Summary of the code quality and key takeaways:


      **Final Rating:** [Rate the code quality and provide reasoning]'
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: code_review
      complexity: medium
      domain: software
      tags:
      - code
      - programming
      - review
      - analysis
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.353452'
      updated_at: '2025-07-31 10:06:57.353457'
      version: 1.0.0
      description: Code analysis and review template
      examples:
      - description: Python function review
        input:
          query: Review this function for potential improvements
          context: "def fibonacci(n):\n    if n <= 1:\n        return n\n    return\
            \ fibonacci(n-1) + fibonacci(n-2)"
        expected_output: '## Code Analysis:


          **Structure & Organization:**

          The function is clearly structured with a simple recursive approach...


          **Potential Issues:**

          This recursive implementation has exponential time complexity...'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 5
        max_length: 1000
        required: true
      context:
        type: str
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  medical_qa:
    template_id: medical_qa
    name: Medical Question Answering
    type: domain_specific
    template: 'Medical Information:

      {{ context | format_documents }}


      Medical Question: {{ query }}


      üè• **MEDICAL ANALYSIS**


      üìã **Clinical Findings**

      ‚Ä¢ What relevant medical information can be extracted from the sources?

      ‚Ä¢ What symptoms, conditions, or treatments are mentioned?

      ‚Ä¢ Are there any diagnostic criteria or clinical indicators noted?


      üî¨ **Medical Analysis**

      ‚Ä¢ Based on the available information, what can be determined?

      ‚Ä¢ How do the findings relate to the specific medical question?

      ‚Ä¢ What are the key medical concepts or mechanisms involved?


      üìö **Evidence-Based Response**

      Based on the provided medical information:


      ‚ö†Ô∏è **IMPORTANT MEDICAL DISCLAIMER**

      ‚Ä¢ This analysis is based solely on the provided documents

      ‚Ä¢ This information is for educational purposes only

      ‚Ä¢ Always consult qualified healthcare professionals for medical advice

      ‚Ä¢ Individual cases may vary significantly and require personalized assessment

      ‚Ä¢ Do not use this information for self-diagnosis or treatment decisions


      **Clinical Response:**'
    input_variables:
    - context
    - query
    optional_variables: []
    metadata:
      use_case: medical_qa
      complexity: high
      domain: medical
      tags:
      - medical
      - healthcare
      - clinical
      - analysis
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.353641'
      updated_at: '2025-07-31 10:06:57.353642'
      version: 1.0.0
      description: Medical document analysis and question answering
      examples:
      - description: Medical symptom inquiry
        input:
          query: What are the common symptoms of hypertension?
          context:
          - title: Hypertension Overview
            content: Hypertension often presents with headaches, dizziness, chest
              pain, and shortness of breath, though many cases are asymptomatic.
        expected_output: 'Based on the medical information provided, here is my analysis:
          **Clinical Findings:** Hypertension commonly presents with headaches, dizziness,
          chest pain, and shortness of breath...'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      query:
        type: str
        min_length: 5
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  llm_judge:
    template_id: llm_judge
    name: LLM as Judge Evaluation
    type: domain_specific
    template: '# LLM Judge Evaluation


      ## Task

      Evaluate the quality of an AI response according to the specified criteria.


      ## Original Query

      {{ original_query }}


      ## Context Information

      {{ context | format_documents }}


      ## AI Response to Evaluate

      {{ response_to_evaluate }}


      ## Evaluation Criteria

      {{ evaluation_criteria }}


      ‚öñÔ∏è **EVALUATION FRAMEWORK**


      üìä **Scoring Dimensions**

      ‚Ä¢ **Relevance** (1-5): How well does the response address the original query?

      ‚Ä¢ **Accuracy** (1-5): How factually correct is the information provided?

      ‚Ä¢ **Completeness** (1-5): Does the response fully answer the question?

      ‚Ä¢ **Clarity** (1-5): How clear and well-structured is the response?

      ‚Ä¢ **Context Usage** (1-5): How effectively does the response use the provided
      context?


      üîç **Detailed Analysis**

      ‚Ä¢ What are the response''s main strengths?

      ‚Ä¢ What are the key weaknesses or areas for improvement?

      ‚Ä¢ Are there any factual errors or misleading statements?

      ‚Ä¢ Does the response appropriately cite or reference the context?

      ‚Ä¢ Is the tone and style appropriate for the query?


      üìà **Scoring Summary**

      ‚Ä¢ Relevance: [Score]/5 - [Brief justification]

      ‚Ä¢ Accuracy: [Score]/5 - [Brief justification]

      ‚Ä¢ Completeness: [Score]/5 - [Brief justification]

      ‚Ä¢ Clarity: [Score]/5 - [Brief justification]

      ‚Ä¢ Context Usage: [Score]/5 - [Brief justification]


      **Overall Score**: [Total]/25

      **Grade**: [A/B/C/D/F]


      **Key Recommendation**: [One actionable improvement suggestion]'
    input_variables:
    - original_query
    - context
    - response_to_evaluate
    - evaluation_criteria
    optional_variables: []
    metadata:
      use_case: llm_evaluation
      complexity: high
      domain: evaluation
      tags:
      - evaluation
      - judge
      - scoring
      - quality_assessment
      - llm_testing
      author: LlamaFarm Team
      created_at: '2025-07-31 10:06:57.353811'
      updated_at: '2025-07-31 10:06:57.353813'
      version: 1.0.0
      description: LLM as Judge template for evaluating AI response quality
      examples:
      - description: Evaluating a medical Q&A response
        input:
          original_query: What are the symptoms of diabetes?
          context:
          - title: Diabetes Overview
            content: Diabetes symptoms include frequent urination, excessive thirst,
              unexplained weight loss, and fatigue.
          response_to_evaluate: Diabetes symptoms include feeling thirsty and urinating
            frequently. You might also lose weight without trying.
          evaluation_criteria: Medical accuracy, completeness, and appropriate medical
            disclaimers
        expected_output: '## Evaluation:

          **Relevance**: 5/5 - Directly addresses the query

          **Accuracy**: 4/5 - Information is correct but incomplete

          **Completeness**: 3/5 - Missing fatigue and other symptoms

          **Overall Score**: 18/25'
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      original_query:
        type: str
        min_length: 5
        max_length: 1000
        required: true
      response_to_evaluate:
        type: str
        min_length: 10
        max_length: 5000
        required: true
      evaluation_criteria:
        type: str
        min_length: 10
        max_length: 1000
        required: true
      context:
        type: list
        required: true
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  tool_execution:
    template_id: tool_execution
    name: Structured Tool Execution
    type: agentic
    template: '# üîß Tool Execution Framework


      ## Current Task

      **Action**: {{action}}

      **Tool**: {{tool_name}}

      **Step**: {{current_step}} of {{total_steps}}


      ## üìã Execution Context


      ### Previous Steps Completed:

      {% if previous_results %}

      {% for result in previous_results %}

      - **Step {{result.step}}**: {{result.action}} ‚Üí {{result.outcome}}

      {% endfor %}

      {% else %}

      - This is the first step

      {% endif %}


      ### Current Tool Information:

      - **Tool Name**: {{tool_name}}

      - **Purpose**: {{tool_purpose}}

      - **Input Format**: {{input_format}}

      - **Expected Output**: {{expected_output}}


      ## üéØ Execution Protocol


      ### 1Ô∏è‚É£ **Pre-Execution Validation**

      - ‚úÖ Confirm tool is appropriate for this task

      - ‚úÖ Validate input parameters are correctly formatted

      - ‚úÖ Check dependencies from previous steps are satisfied

      - ‚úÖ Verify expected output will serve the next step


      ### 2Ô∏è‚É£ **Tool Invocation**

      ```

      Tool: {{tool_name}}

      Input: {{tool_input}}

      Parameters: {{tool_parameters | default(''None'')}}

      Mode: {{execution_mode | default(''standard'')}}

      ```


      ### 3Ô∏è‚É£ **Output Processing**

      - **Capture**: Record the tool''s raw output

      - **Validate**: Ensure output meets expected format

      - **Transform**: Convert output for next step if needed

      - **Store**: Save results for future reference


      ### 4Ô∏è‚É£ **Error Handling**

      {% if error_handling %}

      {{error_handling}}

      {% else %}

      - If tool fails: Log error, attempt retry with modified parameters

      - If output invalid: Validate format, request correction

      - If unexpected result: Analyze cause, adjust approach

      - If critical failure: Escalate to human oversight

      {% endif %}


      ## üìä Quality Checks


      ### Output Validation:

      - **Format Check**: Does output match expected structure?

      - **Content Check**: Is the information relevant and accurate?

      - **Completeness Check**: Are all required fields populated?

      - **Consistency Check**: Does output align with previous steps?


      ### Next Step Preparation:

      {% if next_step %}

      - **Next Action**: {{next_step.action}}

      - **Required Data**: {{next_step.required_data}}

      - **Handoff Format**: {{next_step.input_format}}

      {% else %}

      - This is the final step - prepare summary

      {% endif %}


      ## üîÑ **Execute Tool and Process Result**


      **Step {{current_step}} Result**:

      [Tool execution output will be processed here]


      **Status**: [Success/Failure/Retry Needed]

      **Next Action**: [What to do next based on result]


      ---


      {% if current_step < total_steps %}

      **Ready for Step {{current_step + 1}}**: {{next_step.action if next_step else
      ''Final processing''}}

      {% else %}

      **Workflow Complete**: All {{total_steps}} steps executed successfully

      {% endif %}'
    input_variables:
    - action
    - tool_name
    - current_step
    - total_steps
    optional_variables:
    - previous_results
    - tool_purpose
    - input_format
    - expected_output
    - tool_input
    - tool_parameters
    - execution_mode
    - error_handling
    - next_step
    metadata:
      use_case: Structured execution of individual tools within multi-step workflows
      complexity: medium
      domain: agentic
      tags:
      - tool-execution
      - workflow
      - validation
      - error-handling
      - structured
      author: null
      created_at: '2025-07-31 10:06:57.354300'
      updated_at: '2025-07-31 10:06:57.354301'
      version: 1.0.0
      description: Template for executing individual tools with proper validation
        and error handling
      examples:
      - action: Search for recent AI research papers
        tool_name: academic_search
        current_step: 2
        total_steps: 5
        tool_purpose: Find relevant academic papers on specified topic
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      action:
        type: str
      tool_name:
        type: str
      current_step:
        type: int
      total_steps:
        type: int
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  tool_selection:
    template_id: tool_selection
    name: Intelligent Tool Selection
    type: agentic
    template: "# \U0001F527 Intelligent Tool Selection System\n\n## Task Context\n\
      **Objective**: {{objective}}\n**Domain**: {{domain | default('general')}}\n\
      **Complexity Level**: {{complexity_level | default('medium')}}\n**Available\
      \ Resources**: {{available_resources | default('standard')}}\n**Time Constraints**:\
      \ {{time_constraints | default('normal')}}\n\n## \U0001F6E0Ô∏è Available Tools\
      \ Inventory\n\n{% for tool in available_tools %}\n### {{tool.name}}\n- **Category**:\
      \ {{tool.category}}\n- **Capabilities**: {{tool.capabilities | join(', ')}}\n\
      - **Best For**: {{tool.best_for}}\n- **Limitations**: {{tool.limitations}}\n\
      - **Resource Cost**: {{tool.cost | default('medium')}}\n- **Execution Time**:\
      \ {{tool.execution_time | default('varies')}}\n- **Reliability**: {{tool.reliability\
      \ | default('high')}}\n- **Prerequisites**: {{tool.prerequisites | default('none')}}\n\
      \n{% endfor %}\n\n## \U0001F3AF Tool Selection Framework\n\n### Step 1: Requirement\
      \ Analysis\n**Primary Requirements**:\n{% if primary_requirements %}\n{% for\
      \ req in primary_requirements %}\n- {{req.type}}: {{req.description}} (Priority:\
      \ {{req.priority}})\n{% endfor %}\n{% else %}\n- Analyze what the task specifically\
      \ needs\n- Identify input/output format requirements\n- Determine quality vs\
      \ speed tradeoffs\n- Consider integration complexity\n{% endif %}\n\n**Secondary\
      \ Requirements**:\n{% if secondary_requirements %}\n{% for req in secondary_requirements\
      \ %}\n- {{req.type}}: {{req.description}} (Nice-to-have: {{req.importance}})\n\
      {% endfor %}\n{% else %}\n- Performance optimization needs\n- User experience\
      \ considerations\n- Maintainability requirements\n- Scalability factors\n{%\
      \ endif %}\n\n### Step 2: Tool Compatibility Matrix\n\n| Tool | Capability Match\
      \ | Resource Fit | Time Fit | Integration Ease | Overall Score |\n|------|------------------|--------------|----------|------------------|---------------|\n\
      {% for tool in available_tools %}\n| {{tool.name}} | {{tool.capability_match\
      \ | default('TBD')}} | {{tool.resource_fit | default('TBD')}} | {{tool.time_fit\
      \ | default('TBD')}} | {{tool.integration_ease | default('TBD')}} | {{tool.overall_score\
      \ | default('TBD')}} |\n{% endfor %}\n\n### Step 3: Selection Criteria Weighting\n\
      \n**Criteria Weights** (Total = 100%):\n- **Capability Match**: {{capability_weight\
      \ | default('40%')}}\n- **Resource Efficiency**: {{resource_weight | default('20%')}}\n\
      - **Execution Speed**: {{speed_weight | default('15%')}}\n- **Reliability**:\
      \ {{reliability_weight | default('15%')}}\n- **Integration Ease**: {{integration_weight\
      \ | default('10%')}}\n\n### Step 4: Decision Logic\n\n```\nIF task_type == \"\
      {{task_type | default('analysis')}}\":\n    PREFER tools with: {{preferred_capabilities\
      \ | join(', ') if preferred_capabilities else 'analytical capabilities'}}\n\
      \    \nIF time_constraints == \"tight\":\n    WEIGHT execution_speed *= 2\n\
      \    \nIF resource_constraints == \"limited\":\n    FILTER OUT high_cost_tools\n\
      \    \nIF integration_complexity == \"high\":\n    PREFER tools with: good_apis,\
      \ documentation, support\n```\n\n## \U0001F680 Recommended Tool Selection\n\n\
      ### Primary Tool Choice: {{primary_tool | default('[To be determined]')}}\n\
      **Justification**:\n{% if primary_justification %}\n{{primary_justification}}\n\
      {% else %}\n- Best match for core requirements\n- Optimal resource/performance\
      \ balance\n- Proven reliability for this use case\n- Good integration characteristics\n\
      {% endif %}\n\n**Configuration**:\n- **Parameters**: {{primary_config | default('Default\
      \ settings recommended')}}\n- **Resource Allocation**: {{primary_resources |\
      \ default('Standard allocation')}}\n- **Expected Performance**: {{primary_performance\
      \ | default('High quality results expected')}}\n\n### Backup Tool Choice: {{backup_tool\
      \ | default('[Fallback if needed]')}}\n**Fallback Scenario**: {{fallback_scenario\
      \ | default('If primary tool fails or is unavailable')}}\n**Differences**: {{backup_differences\
      \ | default('May have different performance characteristics')}}\n\n### Tool\
      \ Combination Strategy\n{% if tool_combination %}\n**Multi-Tool Approach**:\
      \ {{tool_combination.strategy}}\n**Workflow**:\n{% for step in tool_combination.steps\
      \ %}\n{{loop.index}}. **{{step.tool}}**: {{step.purpose}}\n   - Input: {{step.input}}\n\
      \   - Output: {{step.output}}\n{% endfor %}\n{% else %}\n**Single Tool Approach**:\
      \ Use primary tool for entire task\n**Rationale**: Task complexity doesn't warrant\
      \ multiple tools\n{% endif %}\n\n## ‚ö†Ô∏è Risk Assessment & Mitigation\n\n### Potential\
      \ Issues:\n{% if potential_issues %}\n{% for issue in potential_issues %}\n\
      - **{{issue.type}}**: {{issue.description}}\n  - **Probability**: {{issue.probability}}\n\
      \  - **Impact**: {{issue.impact}}\n  - **Mitigation**: {{issue.mitigation}}\n\
      {% endfor %}\n{% else %}\n- Tool availability issues\n- Performance degradation\
      \ under load\n- Integration compatibility problems\n- Unexpected output format\
      \ changes\n{% endif %}\n\n### Monitoring Strategy:\n- **Success Metrics**: {{success_metrics\
      \ | default('Output quality, execution time, error rate')}}\n- **Warning Thresholds**:\
      \ {{warning_thresholds | default('Performance 20% below expected')}}\n- **Failover\
      \ Triggers**: {{failover_triggers | default('3 consecutive failures or 50% performance\
      \ drop')}}\n\n## \U0001F4CA Expected Outcomes\n\n### Performance Predictions:\n\
      - **Execution Time**: {{predicted_time | default('Based on tool specifications')}}\n\
      - **Resource Usage**: {{predicted_resources | default('Within allocated limits')}}\n\
      - **Output Quality**: {{predicted_quality | default('High quality expected')}}\n\
      - **Success Probability**: {{success_probability | default('85-95% based on\
      \ tool reliability')}}\n\n### Quality Gates:\n1. **Input Validation**: Ensure\
      \ input meets tool requirements\n2. **Process Monitoring**: Track execution\
      \ progress and performance\n3. **Output Verification**: Validate output format\
      \ and content\n4. **Integration Testing**: Confirm compatibility with downstream\
      \ processes\n\n## \U0001F3AF **Final Tool Selection Decision**\n\n**Selected\
      \ Tool(s)**: {{final_selection | default('[Decision pending analysis]')}}\n\
      **Confidence Level**: {{confidence_level | default('High')}}\n**Implementation\
      \ Priority**: {{implementation_priority | default('Immediate')}}\n\n**Next Steps**:\n\
      1. {{next_step_1 | default('Validate tool availability and configuration')}}\n\
      2. {{next_step_2 | default('Set up monitoring and error handling')}}\n3. {{next_step_3\
      \ | default('Execute initial test run')}}\n4. {{next_step_4 | default('Deploy\
      \ to production workflow')}}\n\n---\n\n**Selection Rationale Summary**: {{selection_summary\
      \ | default('Comprehensive analysis of requirements, capabilities, and constraints\
      \ led to optimal tool choice for this specific task context.')}}"
    input_variables:
    - objective
    - available_tools
    optional_variables:
    - domain
    - complexity_level
    - available_resources
    - time_constraints
    - primary_requirements
    - secondary_requirements
    - task_type
    - preferred_capabilities
    - capability_weight
    - resource_weight
    - speed_weight
    - reliability_weight
    - integration_weight
    - primary_tool
    - primary_justification
    - primary_config
    - primary_resources
    - primary_performance
    - backup_tool
    - fallback_scenario
    - backup_differences
    - tool_combination
    - potential_issues
    - success_metrics
    - warning_thresholds
    - failover_triggers
    - predicted_time
    - predicted_resources
    - predicted_quality
    - success_probability
    - final_selection
    - confidence_level
    - implementation_priority
    - next_step_1
    - next_step_2
    - next_step_3
    - next_step_4
    - selection_summary
    metadata:
      use_case: Intelligently selecting the best tools for specific tasks based on
        requirements and constraints
      complexity: high
      domain: agentic
      tags:
      - tool-selection
      - decision-making
      - optimization
      - analysis
      - strategy
      author: null
      created_at: '2025-07-31 10:06:57.354535'
      updated_at: '2025-07-31 10:06:57.354536'
      version: 1.0.0
      description: Template for systematic tool selection based on task requirements,
        constraints, and available options
      examples:
      - objective: Generate comprehensive market research report
        available_tools:
        - name: web_scraper
          category: data_collection
          capabilities:
          - web_scraping
          - data_extraction
          best_for: gathering online information
        - name: data_analyzer
          category: analysis
          capabilities:
          - statistical_analysis
          - visualization
          best_for: processing large datasets
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      objective:
        type: str
      available_tools:
        type: list
      complexity_level:
        type: str
        enum:
        - low
        - medium
        - high
        - very_high
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  tool_planning:
    template_id: tool_planning
    name: Multi-Step Tool Planning
    type: agentic
    template: "# \U0001F916 Agent Tool Planning System\n\n## Task Overview\n**Goal**:\
      \ {{goal}}\n**Available Tools**: {{available_tools}}\n{% if context %}**Context**:\
      \ {{context}}{% endif %}\n\n## \U0001F4CB Planning Framework\n\n### 1Ô∏è‚É£ **Task\
      \ Analysis**\n- Break down the goal into actionable steps\n- Identify dependencies\
      \ between steps\n- Determine required tools for each step\n\n### 2Ô∏è‚É£ **Tool\
      \ Selection Strategy**\n{% for tool in available_tools %}\n- **{{tool.name}}**:\
      \ {{tool.description}}\n  - Use when: {{tool.use_case}}\n  - Input requirements:\
      \ {{tool.inputs}}\n{% endfor %}\n\n### 3Ô∏è‚É£ **Execution Plan**\nCreate a step-by-step\
      \ plan in this format:\n\n**Step X**: [Action description]\n- Tool: [tool_name]\n\
      - Input: [specific inputs needed]\n- Expected output: [what this step will produce]\n\
      - Dependencies: [what must be completed first]\n\n### 4Ô∏è‚É£ **Contingency Planning**\n\
      - What could go wrong at each step?\n- Alternative approaches if primary plan\
      \ fails\n- Recovery strategies\n\n## \U0001F3AF **Execution Plan**\n[Your detailed\
      \ step-by-step plan here]"
    input_variables:
    - goal
    - available_tools
    optional_variables:
    - context
    - constraints
    - success_criteria
    metadata:
      use_case: Planning multi-step tool usage for AI agents
      complexity: high
      domain: agentic
      tags:
      - agents
      - planning
      - tools
      - multi-step
      - workflow
      author: null
      created_at: '2025-07-31 10:06:57.354688'
      updated_at: '2025-07-31 10:06:57.354689'
      version: 1.0.0
      description: Template for planning complex multi-step workflows with tool usage
      examples:
      - goal: Research and create a market analysis report
        available_tools:
        - name: web_search
          description: Search web for information
          use_case: gathering market data
          inputs: search query
        - name: data_analyzer
          description: Analyze datasets
          use_case: statistical analysis
          inputs: data file
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      goal:
        type: str
      available_tools:
        type: list
      context:
        type: str
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  workflow_orchestration:
    template_id: workflow_orchestration
    name: Complex Workflow Orchestration
    type: agentic
    template: "# \U0001F3BC Workflow Orchestration Engine\n\n## Workflow Overview\n\
      **Workflow Name**: {{workflow_name}}\n**Total Steps**: {{total_steps}}\n**Current\
      \ Status**: {{current_status | default('Initializing')}}\n**Priority Level**:\
      \ {{priority | default('Medium')}}\n\n## \U0001F4CB Workflow Definition\n\n\
      ### Input Requirements:\n{% for input in inputs %}\n- **{{input.name}}**: {{input.description}}\
      \ (Type: {{input.type}}, Required: {{input.required | default('Yes')}})\n{%\
      \ endfor %}\n\n### Expected Outputs:\n{% for output in expected_outputs %}\n\
      - **{{output.name}}**: {{output.description}} (Format: {{output.format}})\n\
      {% endfor %}\n\n## \U0001F504 Step-by-Step Orchestration\n\n{% for step in workflow_steps\
      \ %}\n### Step {{step.id}}: {{step.name}}\n**Type**: {{step.type}}\n**Agent/Tool**:\
      \ {{step.executor}}\n**Dependencies**: {{step.dependencies | join(', ') if step.dependencies\
      \ else 'None'}}\n**Estimated Duration**: {{step.duration | default('Unknown')}}\n\
      \n**Inputs**:\n{% for input in step.inputs %}\n- {{input.name}}: {{input.source}}\
      \ ({{input.type}})\n{% endfor %}\n\n**Processing Logic**:\n{{step.logic}}\n\n\
      **Success Criteria**:\n{% for criteria in step.success_criteria %}\n- {{criteria}}\n\
      {% endfor %}\n\n**Error Handling**:\n- **Retry Logic**: {{step.retry_logic |\
      \ default('Exponential backoff, max 3 attempts')}}\n- **Fallback**: {{step.fallback\
      \ | default('Skip step and continue with degraded functionality')}}\n- **Escalation**:\
      \ {{step.escalation | default('Alert supervisor after 2 failures')}}\n\n**Output\
      \ Specification**:\n{% for output in step.outputs %}\n- {{output.name}}: {{output.description}}\
      \ ‚Üí Goes to: {{output.destination}}\n{% endfor %}\n\n---\n{% endfor %}\n\n##\
      \ \U0001F500 Orchestration Control Flow\n\n### Parallel Execution Opportunities:\n\
      {% if parallel_groups %}\n{% for group in parallel_groups %}\n- **Group {{group.id}}**:\
      \ Steps {{group.steps | join(', ')}} can run simultaneously\n  - Resource requirements:\
      \ {{group.resources}}\n  - Synchronization point: Step {{group.sync_point}}\n\
      {% endfor %}\n{% else %}\n- Analyze steps for parallelization opportunities\n\
      - Identify resource conflicts and dependencies\n- Optimize execution order for\
      \ minimum total time\n{% endif %}\n\n### Critical Path Analysis:\n- **Longest\
      \ sequence**: {{critical_path | join(' ‚Üí ') if critical_path else 'To be determined'}}\n\
      - **Total estimated time**: {{total_estimated_time | default('Sum of critical\
      \ path')}}\n- **Bottleneck steps**: {{bottlenecks | join(', ') if bottlenecks\
      \ else 'To be identified'}}\n\n### Decision Points:\n{% if decision_points %}\n\
      {% for decision in decision_points %}\n- **After Step {{decision.after_step}}**:\
      \ {{decision.condition}}\n  - If True: Continue to Step {{decision.if_true}}\n\
      \  - If False: {{decision.if_false}}\n{% endfor %}\n{% else %}\n- No conditional\
      \ branching in this workflow\n{% endif %}\n\n## \U0001F4CA Monitoring & Control\n\
      \n### Progress Tracking:\n- **Completion Percentage**: {{completion_percentage\
      \ | default('0%')}}\n- **Steps Completed**: {{completed_steps | default('None')}}\
      \ / {{total_steps}}\n- **Current Bottlenecks**: {{current_bottlenecks | default('None\
      \ identified')}}\n- **Resource Utilization**: {{resource_utilization | default('Normal')}}\n\
      \n### Quality Gates:\n{% if quality_gates %}\n{% for gate in quality_gates %}\n\
      - **After Step {{gate.step}}**: {{gate.check}}\n  - Pass Criteria: {{gate.criteria}}\n\
      \  - Fail Action: {{gate.fail_action}}\n{% endfor %}\n{% else %}\n- Quality\
      \ validation after each major milestone\n- Output format verification before\
      \ handoffs\n- Final quality check before workflow completion\n{% endif %}\n\n\
      ### Error Recovery Strategies:\n1. **Step-Level Recovery**: Individual step\
      \ retry with modified parameters\n2. **Workflow-Level Recovery**: Restart from\
      \ last successful checkpoint\n3. **Human Intervention**: Escalate complex errors\
      \ to human operators\n4. **Graceful Degradation**: Complete workflow with reduced\
      \ functionality\n\n## \U0001F3AF Execution Commands\n\n### Start Workflow:\n\
      ```\nSTART_WORKFLOW: {{workflow_name}}\nMODE: {{execution_mode | default('automatic')}}\n\
      PRIORITY: {{priority}}\nINPUTS: [Validated input parameters]\n```\n\n### Monitor\
      \ Progress:\n```\nGET_STATUS: {{workflow_name}}\nDETAIL_LEVEL: {{detail_level\
      \ | default('summary')}}\nINCLUDE_METRICS: {{include_metrics | default('true')}}\n\
      ```\n\n### Handle Interventions:\n```\nPAUSE_WORKFLOW: [At decision points or\
      \ errors]\nRESUME_WORKFLOW: [After manual intervention]\nMODIFY_STEP: [Adjust\
      \ parameters mid-execution]\nSKIP_STEP: [Continue without problematic step]\n\
      ```\n\n## \U0001F680 **Initialize Workflow Execution**\n\n**Pre-flight Checklist**:\n\
      - ‚úÖ All required inputs validated\n- ‚úÖ Agent/tool availability confirmed\n-\
      \ ‚úÖ Resource allocation verified\n- ‚úÖ Error handling procedures activated\n\
      - ‚úÖ Monitoring systems engaged\n\n**Execution Status**: {{execution_status |\
      \ default('Ready to Start')}}\n**Next Action**: {{next_action | default('Begin\
      \ Step 1')}}\n\n---\n\n{% if current_status == 'Running' %}\n**Currently Executing**:\
      \ Step {{current_step}} - {{current_step_name}}\n**Progress**: {{step_progress\
      \ | default('In progress...')}}\n**ETA**: {{estimated_completion | default('Calculating...')}}\n\
      {% elif current_status == 'Completed' %}\n**Workflow Completed Successfully**\
      \ ‚úÖ\n**Total Duration**: {{actual_duration}}\n**Final Outputs**: {{final_outputs\
      \ | join(', ')}}\n{% elif current_status == 'Error' %}\n**Workflow Paused Due\
      \ to Error** ‚ö†Ô∏è\n**Error Location**: Step {{error_step}}\n**Error Details**:\
      \ {{error_details}}\n**Recovery Options**: {{recovery_options | join(', ')}}\n\
      {% endif %}"
    input_variables:
    - workflow_name
    - total_steps
    - inputs
    - expected_outputs
    - workflow_steps
    optional_variables:
    - current_status
    - priority
    - parallel_groups
    - critical_path
    - total_estimated_time
    - bottlenecks
    - decision_points
    - completion_percentage
    - completed_steps
    - current_bottlenecks
    - resource_utilization
    - quality_gates
    - execution_mode
    - detail_level
    - include_metrics
    - execution_status
    - next_action
    - current_step
    - current_step_name
    - step_progress
    - estimated_completion
    - actual_duration
    - final_outputs
    - error_step
    - error_details
    - recovery_options
    metadata:
      use_case: Orchestrating complex multi-step workflows with multiple agents and
        tools
      complexity: high
      domain: agentic
      tags:
      - orchestration
      - workflow
      - multi-step
      - coordination
      - monitoring
      - error-handling
      author: null
      created_at: '2025-07-31 10:06:57.354840'
      updated_at: '2025-07-31 10:06:57.354841'
      version: 1.0.0
      description: Template for managing complex workflows with multiple agents, tools,
        and decision points
      examples:
      - workflow_name: Automated Content Creation Pipeline
        total_steps: 6
        inputs:
        - name: topic
          description: Content topic
          type: string
          required: true
        - name: target_audience
          description: Intended audience
          type: string
          required: true
        expected_outputs:
        - name: final_article
          description: Complete article with images
          format: markdown
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      workflow_name:
        type: str
      total_steps:
        type: int
      inputs:
        type: list
      expected_outputs:
        type: list
      workflow_steps:
        type: list
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  agent_reflection:
    template_id: agent_reflection
    name: Agent Self-Reflection and Learning
    type: agentic
    template: "# \U0001F9E0 Agent Reflection & Learning System\n\n## Task Performance\
      \ Review\n**Completed Task**: {{task_description}}\n**Execution Duration**:\
      \ {{duration | default('Not specified')}}\n**Final Outcome**: {{outcome}}\n\
      **Success Level**: {{success_level | default('Partial Success')}}\n\n## \U0001F4CA\
      \ Performance Analysis\n\n### What Went Well? ‚úÖ\n{% if successes %}\n{% for\
      \ success in successes %}\n- **{{success.category}}**: {{success.description}}\n\
      \  - Impact: {{success.impact}}\n  - Why it worked: {{success.reason}}\n{% endfor\
      \ %}\n{% else %}\n[Analyze successful aspects of the task execution]\n- Tool\
      \ usage effectiveness\n- Problem-solving approach\n- Resource utilization\n\
      - Communication clarity\n{% endif %}\n\n### What Could Be Improved? \U0001F504\
      \n{% if improvements %}\n{% for improvement in improvements %}\n- **{{improvement.area}}**:\
      \ {{improvement.issue}}\n  - Root cause: {{improvement.cause}}\n  - Suggested\
      \ fix: {{improvement.solution}}\n  - Priority: {{improvement.priority}}\n{%\
      \ endfor %}\n{% else %}\n[Identify areas for improvement]\n- Efficiency bottlenecks\n\
      - Decision-making quality\n- Tool selection accuracy\n- Error handling effectiveness\n\
      {% endif %}\n\n### Challenges Encountered \U0001F6A7\n{% if challenges %}\n\
      {% for challenge in challenges %}\n- **Challenge**: {{challenge.description}}\n\
      \  - How handled: {{challenge.response}}\n  - Effectiveness: {{challenge.effectiveness}}\n\
      \  - Alternative approaches: {{challenge.alternatives}}\n{% endfor %}\n{% else\
      \ %}\n[Document significant challenges faced]\n- Unexpected obstacles\n- Resource\
      \ limitations\n- Tool failures or limitations\n- Coordination difficulties\n\
      {% endif %}\n\n## \U0001F3AF Strategy Effectiveness Assessment\n\n### Tool Selection\
      \ & Usage\n{% if tools_used %}\n{% for tool in tools_used %}\n- **{{tool.name}}**\n\
      \  - Appropriateness: {{tool.appropriateness}}/10\n  - Efficiency: {{tool.efficiency}}/10\n\
      \  - Result Quality: {{tool.quality}}/10\n  - Would use again: {{tool.reuse\
      \ | default('Yes')}}\n{% endfor %}\n{% else %}\n[Evaluate tool choices and their\
      \ effectiveness]\n- Were the right tools selected for each task?\n- How efficiently\
      \ were tools utilized?\n- Did tools produce expected quality results?\n{% endif\
      \ %}\n\n### Decision-Making Process\n- **Speed**: How quickly were decisions\
      \ made?\n- **Accuracy**: Were decisions well-informed and correct?\n- **Adaptability**:\
      \ How well did the agent adapt when plans changed?\n- **Risk Assessment**: Were\
      \ potential risks properly evaluated?\n\n## \U0001F4DA Learning Extraction\n\
      \n### Key Insights Gained \U0001F4A1\n{% if insights %}\n{% for insight in insights\
      \ %}\n- **{{insight.topic}}**: {{insight.learning}}\n  - Application: {{insight.application}}\n\
      \  - Confidence: {{insight.confidence}}\n{% endfor %}\n{% else %}\n[Extract\
      \ actionable insights for future tasks]\n- Pattern recognition improvements\n\
      - Strategy refinements\n- Tool usage optimizations\n- Error prevention methods\n\
      {% endif %}\n\n### Updated Mental Models \U0001F9E9\n- What assumptions were\
      \ challenged?\n- How has understanding of the problem domain evolved?\n- What\
      \ new patterns or relationships were discovered?\n- How should approach be modified\
      \ for similar future tasks?\n\n## \U0001F680 Optimization Recommendations\n\n\
      ### For Similar Future Tasks:\n1. **Planning Phase**:\n   - {{planning_recommendations\
      \ | default('Apply learnings to improve initial planning')}}\n   \n2. **Execution\
      \ Phase**:\n   - {{execution_recommendations | default('Implement identified\
      \ efficiency improvements')}}\n   \n3. **Monitoring Phase**:\n   - {{monitoring_recommendations\
      \ | default('Enhance progress tracking and early warning systems')}}\n\n###\
      \ Skill Development Priorities:\n{% if skill_priorities %}\n{% for skill in\
      \ skill_priorities %}\n- **{{skill.name}}**: {{skill.description}} (Priority:\
      \ {{skill.priority}})\n{% endfor %}\n{% else %}\n- [Identify specific skills\
      \ to develop based on performance gaps]\n- Tool mastery improvements\n- Problem\
      \ decomposition techniques\n- Error recovery strategies\n{% endif %}\n\n## \U0001F4C8\
      \ Success Metrics for Next Time\n\n{% if future_metrics %}\n{% for metric in\
      \ future_metrics %}\n- **{{metric.name}}**: {{metric.description}} (Target:\
      \ {{metric.target}})\n{% endfor %}\n{% else %}\n- **Efficiency**: Complete similar\
      \ tasks X% faster\n- **Accuracy**: Achieve higher success rate on first attempt\n\
      - **Resource Usage**: Optimize tool usage for better resource efficiency\n-\
      \ **Quality**: Improve output quality metrics\n{% endif %}\n\n## \U0001F3AF\
      \ **Action Items for Continuous Improvement**\n\n**Immediate Actions** (Next\
      \ 1-3 tasks):\n- [List 2-3 specific improvements to implement immediately]\n\
      \n**Medium-term Development** (Next 5-10 tasks):\n- [List strategic improvements\
      \ requiring more practice]\n\n**Long-term Learning Goals** (Ongoing development):\n\
      - [List fundamental skill areas for continued growth]\n\n---\n\n**Reflection\
      \ Confidence**: {{reflection_confidence | default('80%')}}\n**Ready for Next\
      \ Challenge**: {{readiness_level | default('Yes, with improvements implemented')}}"
    input_variables:
    - task_description
    - outcome
    optional_variables:
    - duration
    - success_level
    - successes
    - improvements
    - challenges
    - tools_used
    - insights
    - planning_recommendations
    - execution_recommendations
    - monitoring_recommendations
    - skill_priorities
    - future_metrics
    - reflection_confidence
    - readiness_level
    metadata:
      use_case: Post-task reflection and learning for AI agents to improve future
        performance
      complexity: high
      domain: agentic
      tags:
      - reflection
      - learning
      - self-improvement
      - performance-analysis
      - optimization
      author: null
      created_at: '2025-07-31 10:06:57.354960'
      updated_at: '2025-07-31 10:06:57.354961'
      version: 1.0.0
      description: Template for agents to analyze their performance and extract learning
        for future improvement
      examples:
      - task_description: Research and create market analysis report for new product
          launch
        outcome: Completed comprehensive 15-page report with market size, competition
          analysis, and recommendations
        success_level: Partial Success
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      task_description:
        type: str
      outcome:
        type: str
      success_level:
        type: str
        enum:
        - Complete Success
        - Partial Success
        - Mixed Results
        - Needs Improvement
    langgraph_config: null
    conditions: {}
    fallback_templates: []
  agent_coordinator:
    template_id: agent_coordinator
    name: Multi-Agent Coordination
    type: agentic
    template: '# üé≠ Multi-Agent Coordination System


      ## Mission Brief

      **Primary Objective**: {{objective}}

      **Available Agents**: {{agents}}

      **Coordination Mode**: {{coordination_mode | default(''collaborative'')}}


      ## ü§ù Agent Roles & Responsibilities


      {% for agent in agents %}

      ### {{agent.name}} Agent

      - **Specialty**: {{agent.specialty}}

      - **Capabilities**: {{agent.capabilities | join('', '')}}

      - **Role in Mission**: {{agent.role}}

      - **Communication Protocol**: {{agent.communication | default(''standard'')}}

      {% endfor %}


      ## üìä Coordination Strategy


      ### Phase 1: Task Distribution

      1. **Analyze** the objective and break into sub-tasks

      2. **Assign** tasks based on agent specialties

      3. **Define** handoff points between agents

      4. **Establish** success criteria for each task


      ### Phase 2: Execution Coordination

      - **Parallel Tasks**: Tasks that can run simultaneously

      - **Sequential Dependencies**: Tasks that must wait for others

      - **Critical Path**: The longest sequence of dependent tasks

      - **Resource Conflicts**: Shared resources that need coordination


      ### Phase 3: Integration & Quality Control

      - **Output Integration**: How agent outputs combine

      - **Quality Checkpoints**: Validation at each stage

      - **Error Handling**: Recovery from agent failures

      - **Final Assembly**: Consolidating results


      ## üîÑ Execution Protocol


      {% if coordination_mode == ''hierarchical'' %}

      **Hierarchical Mode**: Central coordinator assigns and manages tasks

      1. Coordinator analyzes objective

      2. Delegates specific tasks to specialized agents

      3. Monitors progress and adjusts assignments

      4. Integrates outputs into final result

      {% elif coordination_mode == ''collaborative'' %}

      **Collaborative Mode**: Agents negotiate and self-organize

      1. All agents review the objective together

      2. Agents negotiate task assignments based on capabilities

      3. Establish peer-to-peer communication channels

      4. Collaborate on integration and quality control

      {% else %}

      **Sequential Mode**: Agents work in defined sequence

      1. Define the processing pipeline

      2. Each agent processes and passes to next

      3. Maintain state and context through handoffs

      4. Final agent produces integrated result

      {% endif %}


      ## üìù Communication Protocol


      **Status Updates**: {{status_frequency | default(''After each major task'')}}

      **Conflict Resolution**: {{conflict_resolution | default(''Escalate to coordinator'')}}

      **Progress Tracking**: {{progress_tracking | default(''Shared status board'')}}


      ## üéØ Success Metrics


      {% if success_metrics %}

      {% for metric in success_metrics %}

      - **{{metric.name}}**: {{metric.description}} (Target: {{metric.target}})

      {% endfor %}

      {% else %}

      - **Completeness**: All sub-tasks completed successfully

      - **Quality**: Output meets specified criteria

      - **Efficiency**: Completed within time/resource constraints

      - **Coordination**: Smooth handoffs between agents

      {% endif %}


      ## üöÄ **Execute Coordination Plan**

      [Your specific coordination strategy and task assignments here]'
    input_variables:
    - objective
    - agents
    optional_variables:
    - coordination_mode
    - success_metrics
    - status_frequency
    - conflict_resolution
    - progress_tracking
    metadata:
      use_case: Coordinating multiple AI agents working together on complex tasks
      complexity: high
      domain: agentic
      tags:
      - multi-agent
      - coordination
      - collaboration
      - workflow
      - delegation
      author: null
      created_at: '2025-07-31 10:06:57.355060'
      updated_at: '2025-07-31 10:06:57.355061'
      version: 1.0.0
      description: Template for coordinating multiple AI agents with different specialties
      examples:
      - objective: Create comprehensive business plan
        agents:
        - name: Researcher
          specialty: Market Research
          capabilities:
          - web_search
          - data_analysis
          role: Gather market intelligence
        - name: Analyst
          specialty: Financial Analysis
          capabilities:
          - financial_modeling
          - projections
          role: Create financial projections
        - name: Writer
          specialty: Document Creation
          capabilities:
          - writing
          - formatting
          role: Compile final business plan
      performance_notes: null
    global_prompts: []
    preprocessing_steps: []
    postprocessing_steps: []
    validation_rules:
      objective:
        type: str
      agents:
        type: list
      coordination_mode:
        type: str
        enum:
        - hierarchical
        - collaborative
        - sequential
    langgraph_config: null
    conditions: {}
    fallback_templates: []
strategies:
  static_strategy:
    strategy_id: static_strategy
    name: Static Template Selection
    type: static
    description: Always uses the same template
    config:
      default_template: qa_basic
    rules: []
    fallback_template: null
    fallback_chain: []
    variants: []
    traffic_split: {}
    performance_metrics: []
    langgraph_workflow: null
    enabled: true
    priority: 100
    tags: []
    created_by: null
  rule_based_strategy:
    strategy_id: rule_based_strategy
    name: Rule-Based Selection
    type: rule_based
    description: Select templates based on explicit rules
    config: {}
    rules:
    - rule_id: medical_domain_rule
      name: Medical Domain Rule
      description: null
      field: domain
      operator: ==
      value: medical
      template_id: medical_qa
      priority: 10
      enabled: true
      additional_conditions: []
      or_conditions: []
      tags: []
      created_by: null
    - rule_id: summary_rule
      name: Summary Request Rule
      description: null
      field: query_type
      operator: ==
      value: summary
      template_id: summarization
      priority: 30
      enabled: true
      additional_conditions: []
      or_conditions: []
      tags: []
      created_by: null
    - rule_id: complex_analysis_rule
      name: Complex Analysis Rule
      description: null
      field: complexity_level
      operator: ==
      value: high
      template_id: chain_of_thought
      priority: 40
      enabled: true
      additional_conditions: []
      or_conditions: []
      tags: []
      created_by: null
    fallback_template: qa_basic
    fallback_chain: []
    variants: []
    traffic_split: {}
    performance_metrics: []
    langgraph_workflow: null
    enabled: true
    priority: 100
    tags: []
    created_by: null
  context_aware_strategy:
    strategy_id: context_aware_strategy
    name: Context-Aware Selection
    type: context_aware
    description: Intelligent selection based on multiple context factors
    config:
      domain_templates:
        medical: medical_qa
        software: code_analysis
      complexity_templates:
        high: chain_of_thought
        medium: qa_detailed
        low: qa_basic
      intent_templates:
        summarize: summarization
        chat: chat_assistant
    rules: []
    fallback_template: qa_basic
    fallback_chain: []
    variants: []
    traffic_split: {}
    performance_metrics: []
    langgraph_workflow: null
    enabled: true
    priority: 100
    tags: []
    created_by: null
fallback_behavior:
  strategy_fallback_chain:
  - context_aware_strategy
  - rule_based_strategy
  - static_strategy
  template_fallback_chain:
  - qa_basic
  - chat_assistant
  error_handling:
    log_errors: true
    return_error_details: false
integrations:
  rag_system:
    enabled: true
    context_mapping:
      documents: context
      query: query
      domain: domain
  langgraph:
    enabled: false
    workflow_endpoint: http://localhost:8000/workflows
monitoring:
  enabled: true
  metrics:
  - execution_time
  - template_usage
  - error_rate
  - fallback_rate
  logging_level: INFO
environments:
  development:
    monitoring:
      logging_level: DEBUG
    fallback_behavior:
      return_error_details: true
  production:
    monitoring:
      logging_level: WARNING
    global_prompts:
    - global_id: production_disclaimer
      name: Production Disclaimer
      suffix_prompt: '


        ---

        Note: This response was generated by an AI system. Please verify important
        information independently.'
      applies_to:
      - '*'
      priority: 200
      enabled: true
