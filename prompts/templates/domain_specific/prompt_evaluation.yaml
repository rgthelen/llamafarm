template_id: prompt_evaluation
name: Prompt Effectiveness Evaluation
type: domain_specific
template: '# Prompt Effectiveness Evaluation


  ## Prompt Being Evaluated

  ```

  {{ prompt_template }}

  ```


  ## Test Cases and Responses

  {{ test_cases }}


  ## Evaluation Context

  {{ evaluation_context }}


  ðŸ“ **PROMPT EVALUATION FRAMEWORK**


  ðŸŽ¯ **Clarity & Instructions**

  â€¢ **Instruction Clarity**: Are the instructions clear and unambiguous?

  â€¢ **Task Definition**: Is the desired task/output clearly defined?

  â€¢ **Format Specification**: Are output format requirements clearly stated?

  â€¢ **Role Definition**: Is the AI''s role or persona clearly established?


  ðŸ§  **Cognitive Load & Complexity**

  â€¢ **Cognitive Demand**: How much reasoning does the prompt require?

  â€¢ **Context Length**: Is the prompt appropriately concise yet comprehensive?

  â€¢ **Step Breakdown**: Are complex tasks broken into manageable steps?

  â€¢ **Mental Model**: Does the prompt help establish the right mental framework?


  âš¡ **Performance Effectiveness**

  â€¢ **Consistency**: Does the prompt produce consistent outputs across similar inputs?

  â€¢ **Accuracy**: How accurate are the responses generated by this prompt?

  â€¢ **Completeness**: Do responses fully address the intended requirements?

  â€¢ **Reliability**: How reliably does the prompt work across different scenarios?


  ðŸŽ¨ **Design Quality**

  â€¢ **Structure**: Is the prompt well-organized and logically structured?

  â€¢ **Examples**: Are examples provided where helpful?

  â€¢ **Constraints**: Are appropriate constraints and boundaries set?

  â€¢ **Flexibility**: Can the prompt handle variations in input?


  ðŸ“Š **Evaluation Scores**


  **Clarity Metrics:**

  â€¢ Instruction Clarity: [Score 1-5] - [Justification]

  â€¢ Task Definition: [Score 1-5] - [Justification]

  â€¢ Format Specification: [Score 1-5] - [Justification]

  â€¢ Role Definition: [Score 1-5] - [Justification]


  **Effectiveness Metrics:**

  â€¢ Consistency: [Score 1-5] - [Justification]

  â€¢ Accuracy: [Score 1-5] - [Justification]

  â€¢ Completeness: [Score 1-5] - [Justification]

  â€¢ Reliability: [Score 1-5] - [Justification]


  **Design Metrics:**

  â€¢ Structure: [Score 1-5] - [Justification]

  â€¢ Examples Usage: [Score 1-5] - [Justification]

  â€¢ Constraint Setting: [Score 1-5] - [Justification]

  â€¢ Flexibility: [Score 1-5] - [Justification]


  **Overall Prompt Score**: [Total]/60

  **Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) / Very
  Poor (0-19)]


  ðŸ” **Detailed Analysis**


  **What Works Well:**

  â€¢ [Specific strengths of the prompt]


  **Areas for Improvement:**

  â€¢ [Specific weaknesses and issues]


  **Common Response Patterns:**

  â€¢ [Patterns observed in AI responses]


  **Edge Cases & Failure Modes:**

  â€¢ [Scenarios where the prompt fails or underperforms]


  ðŸ’¡ **Optimization Recommendations**


  **High Priority:**

  â€¢ [Most critical improvements needed]


  **Medium Priority:**

  â€¢ [Secondary improvements that would help]


  **Optimization Techniques:**

  â€¢ [Specific prompt engineering techniques to apply]


  **Revised Prompt Suggestion:**

  ```

  [Provide an improved version of the prompt if significant changes are recommended]

  ```'
input_variables:
- prompt_template
- test_cases
- evaluation_context
optional_variables: []
metadata:
  use_case: prompt_optimization
  complexity: high
  domain: evaluation
  description: Comprehensive evaluation template for prompt effectiveness and optimization
  tags:
  - prompt_engineering
  - evaluation
  - optimization
  - testing
  - effectiveness
  author: LlamaFarm Team
  examples:
  - description: Evaluating a summarization prompt
    input:
      prompt_template: 'Summarize the following text in 3 sentences: {{ text }}'
      test_cases: 'Test 1: Long research paper -> Generated a 2-sentence summary missing
        key points

        Test 2: News article -> Generated a 4-sentence summary with good coverage

        Test 3: Technical documentation -> Generated unclear summary with jargon'
      evaluation_context: Testing for general document summarization across different
        text types
    expected_output: '**Overall Prompt Score**: 35/60 - Fair

      **Key Issues**: Inconsistent length adherence, lacks domain adaptation

      **Recommendations**: Add length constraints, specify clarity requirements, include
      examples'
validation_rules:
  prompt_template:
    type: str
    min_length: 10
    max_length: 5000
    required: true
  test_cases:
    type: str
    min_length: 20
    max_length: 10000
    required: true
  evaluation_context:
    type: str
    min_length: 10
    max_length: 1000
    required: true
