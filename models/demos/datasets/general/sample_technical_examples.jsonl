{"instruction": "Show me how to create a basic LlamaFarm configuration file", "input": "", "output": "Here's a basic LlamaFarm configuration:\n\n```yaml\nbase_model:\n  name: \"llama-7b\"\n  huggingface_id: \"meta-llama/Llama-2-7b-hf\"\n  cache_dir: \"./models\"\n\nmethod:\n  type: \"lora\"\n  r: 16\n  alpha: 32\n  dropout: 0.1\n  target_modules: [\"q_proj\", \"v_proj\"]\n\nframework:\n  type: \"pytorch\"\n  use_fast_tokenizer: true\n\ntraining_args:\n  output_dir: \"./fine_tuned_model\"\n  num_train_epochs: 3\n  per_device_train_batch_size: 4\n  learning_rate: 2e-4\n  max_seq_length: 512\n\ndataset:\n  path: \"./training_data.jsonl\"\n  data_format: \"alpaca\"\n```"}
{"instruction": "How do I start fine-tuning with a specific strategy?", "input": "", "output": "To start fine-tuning with a strategy:\n\n```bash\n# Using a predefined strategy\npython cli.py finetune start --strategy mac_m1_lora --dataset ./my_data.jsonl\n\n# Or with a custom config file\npython cli.py finetune start --config ./my_config.yaml\n\n# Monitor progress\npython cli.py finetune status\n\n# List available strategies\npython cli.py finetune list-strategies\n```"}
{"instruction": "Write a Python script to validate my training dataset", "input": "", "output": "Here's a script to validate your training dataset:\n\n```python\nfrom components.data.processors import AlpacaProcessor\nfrom pathlib import Path\n\ndef validate_dataset(dataset_path):\n    processor = AlpacaProcessor({})\n    errors = processor.validate_dataset(dataset_path)\n    \n    if errors:\n        print(f\"âŒ Dataset validation failed:\")\n        for error in errors:\n            print(f\"  â€¢ {error}\")\n        return False\n    else:\n        print(f\"âœ… Dataset validation passed\")\n        return True\n\n# Usage\nif __name__ == \"__main__\":\n    dataset_file = \"./my_training_data.jsonl\"\n    is_valid = validate_dataset(dataset_file)\n```"}
{"instruction": "How do I create a custom fine-tuning strategy?", "input": "", "output": "Create a custom strategy by making a YAML file in the strategies directory:\n\n```yaml\n# strategies/my_custom_strategy.yaml\nname: \"my_custom_strategy\"\ndescription: \"Custom strategy for my specific use case\"\n\nhardware_requirements:\n  type: \"gpu\"\n  memory_gb: 24\n  gpu_memory_gb: 16\n\nbase_model:\n  name: \"custom-llama\"\n  huggingface_id: \"meta-llama/Llama-2-13b-hf\"\n  torch_dtype: \"float16\"\n\nmethod:\n  type: \"lora\"\n  r: 32\n  alpha: 64\n  dropout: 0.05\n  target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\ntraining_args:\n  num_train_epochs: 5\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 8\n  learning_rate: 1e-4\n  warmup_ratio: 0.1\n  lr_scheduler_type: \"cosine\"\n  optim: \"adamw_torch\"\n  bf16: true\n```\n\nThen use it: `python cli.py finetune start --strategy my_custom_strategy`"}
{"instruction": "Show me how to implement a custom data processor", "input": "", "output": "Here's how to create a custom data processor:\n\n```python\nfrom components.core.base import BaseDataProcessor\nfrom datasets import Dataset\nimport json\nfrom pathlib import Path\n\nclass CustomJSONProcessor(BaseDataProcessor):\n    \"\"\"Custom processor for your specific JSON format.\"\"\"\n    \n    def process_dataset(self, dataset_path):\n        path = Path(dataset_path)\n        \n        # Load your custom format\n        with open(path, 'r') as f:\n            data = json.load(f)\n        \n        # Convert to training format\n        processed_data = []\n        for item in data:\n            # Adapt this to your data structure\n            text = f\"Question: {item['question']}\\nAnswer: {item['answer']}\"\n            processed_data.append({\"text\": text})\n        \n        return Dataset.from_list(processed_data)\n    \n    def validate_dataset(self, dataset_path):\n        errors = []\n        try:\n            with open(dataset_path, 'r') as f:\n                data = json.load(f)\n            \n            if not isinstance(data, list):\n                errors.append(\"Data must be a list\")\n            \n            for i, item in enumerate(data[:10]):  # Check first 10\n                if \"question\" not in item:\n                    errors.append(f\"Item {i} missing 'question' field\")\n                if \"answer\" not in item:\n                    errors.append(f\"Item {i} missing 'answer' field\")\n                    \n        except Exception as e:\n            errors.append(f\"Failed to load JSON: {e}\")\n        \n        return errors\n```"}
{"instruction": "How do I monitor training progress programmatically?", "input": "", "output": "Here's how to monitor training progress:\n\n```python\nimport time\nimport json\nfrom pathlib import Path\nfrom components.core.factory import FineTunerFactory\n\ndef monitor_training(config, check_interval=30):\n    \"\"\"Monitor training progress.\"\"\"\n    tuner = FineTunerFactory.create(config)\n    \n    # Start training in background\n    job = tuner.start_training()\n    \n    while not job.is_complete():\n        status = job.get_status()\n        print(f\"Step: {status['step']}, Loss: {status['loss']:.4f}\")\n        \n        # Check for logs\n        log_file = Path(config.training_args['output_dir']) / \"training.log\"\n        if log_file.exists():\n            with open(log_file, 'r') as f:\n                recent_logs = f.readlines()[-5:]  # Last 5 lines\n                for log in recent_logs:\n                    print(f\"LOG: {log.strip()}\")\n        \n        time.sleep(check_interval)\n    \n    print(\"Training completed!\")\n    return job.get_final_metrics()\n```"}
{"instruction": "Show me how to test a fine-tuned model", "input": "", "output": "Here's how to test your fine-tuned model:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndef test_finetuned_model(model_path, test_prompts):\n    \"\"\"Test a fine-tuned model with sample prompts.\"\"\"\n    \n    # Load the fine-tuned model\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    \n    results = []\n    \n    for prompt in test_prompts:\n        print(f\"\\nðŸ”¤ Testing prompt: {prompt}\")\n        \n        # Tokenize input\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        \n        # Generate response\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs.input_ids,\n                max_new_tokens=150,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode response\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response[len(prompt):].strip()\n        \n        print(f\"ðŸ¤– Response: {response}\")\n        results.append({\"prompt\": prompt, \"response\": response})\n    \n    return results\n\n# Usage example\ntest_prompts = [\n    \"What is LlamaFarm?\",\n    \"How do I start fine-tuning?\",\n    \"Explain LoRA fine-tuning\"\n]\n\nresults = test_finetuned_model(\"./fine_tuned_model\", test_prompts)\n```"}
{"instruction": "How do I compare model performance before and after fine-tuning?", "input": "", "output": "Here's a script to compare model performance:\n\n```python\nimport json\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef compare_models(base_model_path, finetuned_model_path, test_data):\n    \"\"\"Compare base model vs fine-tuned model performance.\"\"\"\n    \n    def load_model(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=\"auto\"\n        )\n        return tokenizer, model\n    \n    def generate_response(model, tokenizer, prompt):\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs.input_ids,\n                max_new_tokens=100,\n                temperature=0.1,  # Low temperature for consistency\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Load both models\n    base_tokenizer, base_model = load_model(base_model_path)\n    ft_tokenizer, ft_model = load_model(finetuned_model_path)\n    \n    results = []\n    \n    for item in test_data:\n        prompt = item['prompt']\n        expected = item['expected_response']\n        \n        # Generate responses\n        base_response = generate_response(base_model, base_tokenizer, prompt)\n        ft_response = generate_response(ft_model, ft_tokenizer, prompt)\n        \n        # Simple similarity scoring (you can use more sophisticated metrics)\n        base_score = calculate_similarity(base_response, expected)\n        ft_score = calculate_similarity(ft_response, expected)\n        \n        results.append({\n            'prompt': prompt,\n            'expected': expected,\n            'base_response': base_response,\n            'ft_response': ft_response,\n            'base_score': base_score,\n            'ft_score': ft_score,\n            'improvement': ft_score - base_score\n        })\n    \n    # Calculate overall metrics\n    avg_base_score = np.mean([r['base_score'] for r in results])\n    avg_ft_score = np.mean([r['ft_score'] for r in results])\n    avg_improvement = np.mean([r['improvement'] for r in results])\n    \n    print(f\"ðŸ“Š Performance Comparison:\")\n    print(f\"Base Model Average Score: {avg_base_score:.3f}\")\n    print(f\"Fine-tuned Model Average Score: {avg_ft_score:.3f}\")\n    print(f\"Average Improvement: {avg_improvement:.3f}\")\n    \n    return results\n\ndef calculate_similarity(response, expected):\n    \"\"\"Simple similarity calculation - replace with your preferred metric.\"\"\"\n    # This is a placeholder - use BLEU, ROUGE, or semantic similarity\n    response_words = set(response.lower().split())\n    expected_words = set(expected.lower().split())\n    if not expected_words:\n        return 0.0\n    return len(response_words.intersection(expected_words)) / len(expected_words)\n```"}
{"instruction": "How do I set up distributed training across multiple GPUs?", "input": "", "output": "Here's how to configure distributed training:\n\n```yaml\n# config_distributed.yaml\nbase_model:\n  name: \"llama-13b\"\n  huggingface_id: \"meta-llama/Llama-2-13b-hf\"\n\nmethod:\n  type: \"lora\"\n  r: 64\n  alpha: 128\n\nframework:\n  type: \"pytorch\"\n  distributed: true\n  world_size: 4  # Number of GPUs\n  \ntraining_args:\n  output_dir: \"./distributed_model\"\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 4\n  dataloader_num_workers: 4\n  ddp_find_unused_parameters: false\n  ddp_backend: \"nccl\"\n  \n# Environment setup script\nenv:\n  CUDA_VISIBLE_DEVICES: \"0,1,2,3\"\n  NCCL_DEBUG: \"INFO\"\n```\n\n```bash\n# Launch distributed training\ntorchrun --nproc_per_node=4 --nnodes=1 \\\n  cli.py finetune start --config config_distributed.yaml\n\n# Or using the LlamaFarm CLI\npython cli.py finetune start --config config_distributed.yaml --distributed\n```"}
{"instruction": "Show me how to implement custom evaluation metrics", "input": "", "output": "Here's how to implement custom evaluation metrics:\n\n```python\nfrom typing import Dict, List, Any\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nimport re\n\nclass CustomEvaluator:\n    \"\"\"Custom evaluator for domain-specific metrics.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.metrics = {\n            'accuracy': self.calculate_accuracy,\n            'exact_match': self.calculate_exact_match,\n            'semantic_similarity': self.calculate_semantic_similarity,\n            'task_specific': self.calculate_task_specific_metric\n        }\n    \n    def evaluate(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n        \"\"\"Evaluate predictions against references.\"\"\"\n        results = {}\n        \n        for metric_name, metric_func in self.metrics.items():\n            if self.config.get(f'use_{metric_name}', True):\n                try:\n                    score = metric_func(predictions, references)\n                    results[metric_name] = score\n                except Exception as e:\n                    print(f\"Warning: Failed to calculate {metric_name}: {e}\")\n                    results[metric_name] = 0.0\n        \n        return results\n    \n    def calculate_accuracy(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Calculate token-level accuracy.\"\"\"\n        correct = 0\n        total = 0\n        \n        for pred, ref in zip(predictions, references):\n            pred_tokens = pred.lower().split()\n            ref_tokens = ref.lower().split()\n            \n            for p_token, r_token in zip(pred_tokens, ref_tokens):\n                if p_token == r_token:\n                    correct += 1\n                total += 1\n        \n        return correct / total if total > 0 else 0.0\n    \n    def calculate_exact_match(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Calculate exact match percentage.\"\"\"\n        matches = sum(1 for pred, ref in zip(predictions, references) \n                     if pred.strip().lower() == ref.strip().lower())\n        return matches / len(predictions) if predictions else 0.0\n    \n    def calculate_semantic_similarity(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Calculate semantic similarity (placeholder for actual implementation).\"\"\"\n        # This would typically use sentence transformers or similar\n        similarities = []\n        \n        for pred, ref in zip(predictions, references):\n            # Simple word overlap as placeholder\n            pred_words = set(pred.lower().split())\n            ref_words = set(ref.lower().split())\n            \n            if not ref_words:\n                similarities.append(0.0)\n            else:\n                overlap = len(pred_words.intersection(ref_words))\n                similarity = overlap / len(ref_words)\n                similarities.append(similarity)\n        \n        return np.mean(similarities)\n    \n    def calculate_task_specific_metric(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Calculate task-specific metric (customize for your use case).\"\"\"\n        # Example: For code generation, check if code compiles\n        # Example: For math problems, check if final answer is correct\n        # Example: For QA, check if key facts are present\n        \n        task_type = self.config.get('task_type', 'general')\n        \n        if task_type == 'code':\n            return self._evaluate_code_generation(predictions, references)\n        elif task_type == 'math':\n            return self._evaluate_math_problems(predictions, references)\n        elif task_type == 'qa':\n            return self._evaluate_qa_responses(predictions, references)\n        else:\n            return self.calculate_semantic_similarity(predictions, references)\n    \n    def _evaluate_code_generation(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Evaluate code generation quality.\"\"\"\n        scores = []\n        for pred, ref in zip(predictions, references):\n            # Check for syntax elements, function definitions, etc.\n            score = 0.0\n            if 'def ' in pred or 'class ' in pred:\n                score += 0.3\n            if pred.count('(') == pred.count(')'):\n                score += 0.2\n            if pred.count('{') == pred.count('}'):\n                score += 0.2\n            if any(keyword in pred for keyword in ['return', 'print', 'if', 'for']):\n                score += 0.3\n            scores.append(min(score, 1.0))\n        return np.mean(scores)\n    \n    def _evaluate_math_problems(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Evaluate math problem solutions.\"\"\"\n        scores = []\n        for pred, ref in zip(predictions, references):\n            # Extract numerical answers\n            pred_numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', pred)\n            ref_numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', ref)\n            \n            if pred_numbers and ref_numbers:\n                # Compare final numerical answers\n                try:\n                    pred_answer = float(pred_numbers[-1])\n                    ref_answer = float(ref_numbers[-1])\n                    score = 1.0 if abs(pred_answer - ref_answer) < 0.01 else 0.0\n                except ValueError:\n                    score = 0.0\n            else:\n                score = 0.0\n            scores.append(score)\n        return np.mean(scores)\n    \n    def _evaluate_qa_responses(self, predictions: List[str], references: List[str]) -> float:\n        \"\"\"Evaluate QA response quality.\"\"\"\n        scores = []\n        for pred, ref in zip(predictions, references):\n            # Check if key entities/facts from reference appear in prediction\n            ref_entities = re.findall(r'\\b[A-Z][a-z]+\\b', ref)  # Simple NER\n            pred_lower = pred.lower()\n            \n            if ref_entities:\n                matches = sum(1 for entity in ref_entities \n                            if entity.lower() in pred_lower)\n                score = matches / len(ref_entities)\n            else:\n                score = self.calculate_semantic_similarity([pred], [ref])\n            \n            scores.append(score)\n        return np.mean(scores)\n\n# Usage example\nevaluator = CustomEvaluator({\n    'task_type': 'qa',\n    'use_accuracy': True,\n    'use_exact_match': True,\n    'use_semantic_similarity': True,\n    'use_task_specific': True\n})\n\npredictions = [\"LlamaFarm is an AI model management system.\"]\nreferences = [\"LlamaFarm is a comprehensive AI model management platform.\"]\n\nmetrics = evaluator.evaluate(predictions, references)\nprint(f\"Evaluation Results: {metrics}\")\n```"}
{"instruction": "How do I implement checkpoint resuming for interrupted training?", "input": "", "output": "Here's how to implement checkpoint resuming:\n\n```python\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\nfrom transformers import TrainerCallback, TrainerState, TrainerControl\n\nclass CheckpointManager:\n    \"\"\"Manages training checkpoints and resuming.\"\"\"\n    \n    def __init__(self, output_dir: str):\n        self.output_dir = Path(output_dir)\n        self.checkpoint_info_file = self.output_dir / \"checkpoint_info.json\"\n    \n    def save_checkpoint_info(self, step: int, epoch: float, metrics: Dict[str, Any]):\n        \"\"\"Save checkpoint metadata.\"\"\"\n        info = {\n            \"step\": step,\n            \"epoch\": epoch,\n            \"metrics\": metrics,\n            \"timestamp\": time.time()\n        }\n        \n        self.checkpoint_info_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(self.checkpoint_info_file, 'w') as f:\n            json.dump(info, f, indent=2)\n    \n    def find_latest_checkpoint(self) -> Optional[str]:\n        \"\"\"Find the latest checkpoint directory.\"\"\"\n        checkpoint_dirs = []\n        \n        for item in self.output_dir.iterdir():\n            if item.is_dir() and item.name.startswith(\"checkpoint-\"):\n                try:\n                    step = int(item.name.split(\"-\")[1])\n                    checkpoint_dirs.append((step, str(item)))\n                except ValueError:\n                    continue\n        \n        if checkpoint_dirs:\n            # Return the checkpoint with the highest step number\n            checkpoint_dirs.sort(key=lambda x: x[0])\n            return checkpoint_dirs[-1][1]\n        \n        return None\n    \n    def get_checkpoint_info(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Get checkpoint information.\"\"\"\n        if self.checkpoint_info_file.exists():\n            with open(self.checkpoint_info_file, 'r') as f:\n                return json.load(f)\n        return None\n\nclass ResumeTrainingCallback(TrainerCallback):\n    \"\"\"Callback to handle checkpoint saving and resuming.\"\"\"\n    \n    def __init__(self, checkpoint_manager: CheckpointManager):\n        self.checkpoint_manager = checkpoint_manager\n    \n    def on_save(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n        \"\"\"Called when a checkpoint is saved.\"\"\"\n        if state.log_history:\n            latest_metrics = state.log_history[-1]\n        else:\n            latest_metrics = {}\n        \n        self.checkpoint_manager.save_checkpoint_info(\n            step=state.global_step,\n            epoch=state.epoch,\n            metrics=latest_metrics\n        )\n\ndef resume_training_from_checkpoint(config, checkpoint_dir: Optional[str] = None):\n    \"\"\"Resume training from a checkpoint.\"\"\"\n    from components.core.factory import FineTunerFactory\n    \n    checkpoint_manager = CheckpointManager(config.training_args[\"output_dir\"])\n    \n    if checkpoint_dir is None:\n        checkpoint_dir = checkpoint_manager.find_latest_checkpoint()\n    \n    if checkpoint_dir:\n        print(f\"ðŸ”„ Resuming training from checkpoint: {checkpoint_dir}\")\n        \n        # Get checkpoint info\n        checkpoint_info = checkpoint_manager.get_checkpoint_info()\n        if checkpoint_info:\n            print(f\"ðŸ“Š Previous training state:\")\n            print(f\"   Step: {checkpoint_info['step']}\")\n            print(f\"   Epoch: {checkpoint_info['epoch']:.2f}\")\n            if 'train_loss' in checkpoint_info['metrics']:\n                print(f\"   Loss: {checkpoint_info['metrics']['train_loss']:.4f}\")\n        \n        # Update config to resume from checkpoint\n        config.training_args[\"resume_from_checkpoint\"] = checkpoint_dir\n    else:\n        print(\"ðŸ†• No checkpoint found, starting fresh training\")\n    \n    # Create tuner and add callback\n    tuner = FineTunerFactory.create(config)\n    \n    # Add checkpoint callback if supported\n    if hasattr(tuner, 'add_callback'):\n        callback = ResumeTrainingCallback(checkpoint_manager)\n        tuner.add_callback(callback)\n    \n    return tuner\n\n# Example usage in training script\ndef main():\n    from components.core.base import FineTuningConfig\n    \n    config = FineTuningConfig(\n        base_model={\"name\": \"gpt2\", \"huggingface_id\": \"gpt2\"},\n        method={\"type\": \"lora\", \"r\": 16},\n        framework={\"type\": \"pytorch\"},\n        training_args={\n            \"output_dir\": \"./checkpoints\",\n            \"save_steps\": 100,\n            \"save_total_limit\": 3,  # Keep only 3 most recent checkpoints\n            \"logging_steps\": 10,\n            \"num_train_epochs\": 5\n        },\n        dataset={\"path\": \"./data.jsonl\", \"data_format\": \"alpaca\"}\n    )\n    \n    try:\n        # Try to resume from checkpoint\n        tuner = resume_training_from_checkpoint(config)\n        job = tuner.start_training()\n        \n        # Wait for completion\n        while not job.is_complete():\n            time.sleep(30)\n            status = job.get_status()\n            print(f\"Training progress: Step {status.get('step', 0)}\")\n        \n        print(\"âœ… Training completed successfully!\")\n        \n    except KeyboardInterrupt:\n        print(\"\\nâ¸ï¸  Training interrupted. You can resume later using the same command.\")\n    except Exception as e:\n        print(f\"âŒ Training failed: {e}\")\n        # Checkpoint should still be available for resuming\n\nif __name__ == \"__main__\":\n    main()\n```"}
{"instruction": "Show me how to implement model quantization for deployment", "input": "", "output": "Here's how to implement model quantization:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom pathlib import Path\nimport time\n\nclass ModelQuantizer:\n    \"\"\"Quantize models for efficient deployment.\"\"\"\n    \n    def __init__(self, model_path: str):\n        self.model_path = Path(model_path)\n        self.tokenizer = None\n        self.model = None\n    \n    def quantize_int8(self, output_path: str):\n        \"\"\"Quantize model to INT8.\"\"\"\n        print(\"ðŸ”§ Loading model for INT8 quantization...\")\n        \n        # Configure INT8 quantization\n        quantization_config = BitsAndBytesConfig(\n            load_in_8bit=True,\n            llm_int8_enable_fp32_cpu_offload=True,\n            llm_int8_has_fp16_weight=False\n        )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        \n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_path,\n            quantization_config=quantization_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n        \n        # Save quantized model\n        output_path = Path(output_path)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        self.model.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n        \n        print(f\"âœ… INT8 quantized model saved to {output_path}\")\n        return output_path\n    \n    def quantize_int4(self, output_path: str):\n        \"\"\"Quantize model to INT4.\"\"\"\n        print(\"ðŸ”§ Loading model for INT4 quantization...\")\n        \n        # Configure INT4 quantization\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        \n        # Load model with quantization\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_path,\n            quantization_config=quantization_config,\n            device_map=\"auto\"\n        )\n        \n        # Save quantized model\n        output_path = Path(output_path)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        self.model.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n        \n        print(f\"âœ… INT4 quantized model saved to {output_path}\")\n        return output_path\n    \n    def benchmark_model(self, test_prompts, max_new_tokens=100):\n        \"\"\"Benchmark model performance.\"\"\"\n        if not self.model or not self.tokenizer:\n            raise ValueError(\"Model not loaded. Call quantize_* method first.\")\n        \n        print(f\"ðŸ“Š Benchmarking model with {len(test_prompts)} prompts...\")\n        \n        total_time = 0\n        total_tokens = 0\n        \n        for i, prompt in enumerate(test_prompts):\n            start_time = time.time()\n            \n            # Tokenize input\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n            input_length = inputs.input_ids.shape[1]\n            \n            # Generate response\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs.input_ids,\n                    max_new_tokens=max_new_tokens,\n                    do_sample=True,\n                    temperature=0.7,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Calculate metrics\n            end_time = time.time()\n            generation_time = end_time - start_time\n            output_length = outputs.shape[1] - input_length\n            tokens_per_second = output_length / generation_time\n            \n            total_time += generation_time\n            total_tokens += output_length\n            \n            print(f\"  Prompt {i+1}: {tokens_per_second:.2f} tokens/sec\")\n        \n        avg_tokens_per_sec = total_tokens / total_time\n        print(f\"\\nðŸ“ˆ Average Performance: {avg_tokens_per_sec:.2f} tokens/sec\")\n        \n        return {\n            \"avg_tokens_per_second\": avg_tokens_per_sec,\n            \"total_time\": total_time,\n            \"total_tokens\": total_tokens\n        }\n\ndef compare_quantization_methods(model_path: str, test_prompts):\n    \"\"\"Compare different quantization methods.\"\"\"\n    results = {}\n    \n    # Test original model (FP16)\n    print(\"\\nðŸ” Testing Original Model (FP16)\")\n    print(\"=\" * 50)\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    \n    quantizer = ModelQuantizer(model_path)\n    quantizer.model = model\n    quantizer.tokenizer = tokenizer\n    \n    results[\"fp16\"] = quantizer.benchmark_model(test_prompts)\n    \n    # Test INT8 quantization\n    print(\"\\nðŸ” Testing INT8 Quantization\")\n    print(\"=\" * 50)\n    \n    quantizer_int8 = ModelQuantizer(model_path)\n    int8_path = quantizer_int8.quantize_int8(\"./quantized_int8\")\n    results[\"int8\"] = quantizer_int8.benchmark_model(test_prompts)\n    \n    # Test INT4 quantization\n    print(\"\\nðŸ” Testing INT4 Quantization\")\n    print(\"=\" * 50)\n    \n    quantizer_int4 = ModelQuantizer(model_path)\n    int4_path = quantizer_int4.quantize_int4(\"./quantized_int4\")\n    results[\"int4\"] = quantizer_int4.benchmark_model(test_prompts)\n    \n    # Print comparison\n    print(\"\\nðŸ“Š Quantization Comparison\")\n    print(\"=\" * 50)\n    for method, metrics in results.items():\n        print(f\"{method.upper():>6}: {metrics['avg_tokens_per_second']:>8.2f} tokens/sec\")\n    \n    return results\n\n# Usage example\nif __name__ == \"__main__\":\n    model_path = \"./fine_tuned_model\"\n    test_prompts = [\n        \"What is LlamaFarm?\",\n        \"How do I start fine-tuning?\",\n        \"Explain the benefits of LoRA.\"\n    ]\n    \n    comparison_results = compare_quantization_methods(model_path, test_prompts)\n```"}
{"instruction": "How do I implement a custom training loop with advanced features?", "input": "", "output": "Here's how to implement a custom training loop:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport wandb\nimport json\nfrom pathlib import Path\nimport numpy as np\nfrom typing import Dict, List, Optional, Callable\n\nclass AdvancedTrainer:\n    \"\"\"Advanced custom trainer with full control over training loop.\"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        train_dataloader: DataLoader,\n        val_dataloader: Optional[DataLoader] = None,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n        config: Optional[Dict] = None\n    ):\n        self.model = model\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.config = config or {}\n        \n        # Training settings\n        self.epochs = self.config.get('epochs', 3)\n        self.gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)\n        self.max_grad_norm = self.config.get('max_grad_norm', 1.0)\n        self.logging_steps = self.config.get('logging_steps', 100)\n        self.eval_steps = self.config.get('eval_steps', 500)\n        self.save_steps = self.config.get('save_steps', 1000)\n        self.output_dir = Path(self.config.get('output_dir', './checkpoints'))\n        \n        # Initialize optimizer and scheduler\n        if optimizer is None:\n            self.optimizer = torch.optim.AdamW(\n                self.model.parameters(),\n                lr=self.config.get('learning_rate', 2e-5),\n                weight_decay=self.config.get('weight_decay', 0.01)\n            )\n        else:\n            self.optimizer = optimizer\n            \n        if scheduler is None and self.config.get('use_scheduler', True):\n            total_steps = len(self.train_dataloader) * self.epochs // self.gradient_accumulation_steps\n            warmup_steps = int(total_steps * self.config.get('warmup_ratio', 0.1))\n            self.scheduler = get_linear_schedule_with_warmup(\n                self.optimizer,\n                num_warmup_steps=warmup_steps,\n                num_training_steps=total_steps\n            )\n        else:\n            self.scheduler = scheduler\n        \n        # Training state\n        self.global_step = 0\n        self.epoch = 0\n        self.best_metric = float('inf') if self.config.get('metric_for_best_model') == 'loss' else 0\n        self.training_history = []\n        \n        # Advanced features\n        self.use_mixed_precision = self.config.get('fp16', False)\n        self.early_stopping_patience = self.config.get('early_stopping_patience', None)\n        self.early_stopping_counter = 0\n        \n        # Initialize mixed precision\n        if self.use_mixed_precision:\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n        # Initialize wandb if configured\n        if self.config.get('use_wandb', False):\n            wandb.init(\n                project=self.config.get('wandb_project', 'llamafarm-finetuning'),\n                config=self.config\n            )\n    \n    def train(self):\n        \"\"\"Main training loop.\"\"\"\n        print(f\"ðŸš€ Starting training for {self.epochs} epochs...\")\n        self.model.train()\n        \n        for epoch in range(self.epochs):\n            self.epoch = epoch\n            print(f\"\\nðŸ“š Epoch {epoch + 1}/{self.epochs}\")\n            \n            epoch_loss = self._train_epoch()\n            \n            # Validation\n            if self.val_dataloader is not None:\n                val_metrics = self._validate()\n                print(f\"ðŸ“Š Validation metrics: {val_metrics}\")\n                \n                # Early stopping check\n                if self._should_stop_early(val_metrics):\n                    print(\"ðŸ›‘ Early stopping triggered\")\n                    break\n            \n            # Save checkpoint\n            if (epoch + 1) % self.config.get('save_epochs', 1) == 0:\n                self._save_checkpoint(f\"epoch-{epoch + 1}\")\n        \n        print(\"âœ… Training completed!\")\n        \n        # Save final model\n        self._save_model(\"final\")\n        \n        return self.training_history\n    \n    def _train_epoch(self) -> float:\n        \"\"\"Train for one epoch.\"\"\"\n        total_loss = 0\n        num_batches = len(self.train_dataloader)\n        \n        progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {self.epoch + 1}\")\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            loss = self._train_step(batch)\n            total_loss += loss\n            \n            # Update progress bar\n            avg_loss = total_loss / (batch_idx + 1)\n            progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n            \n            # Logging\n            if self.global_step % self.logging_steps == 0:\n                self._log_metrics({\n                    'train_loss': loss,\n                    'learning_rate': self.optimizer.param_groups[0]['lr'],\n                    'epoch': self.epoch,\n                    'global_step': self.global_step\n                })\n            \n            # Validation during training\n            if (self.val_dataloader is not None and \n                self.global_step % self.eval_steps == 0 and \n                self.global_step > 0):\n                val_metrics = self._validate()\n                self._log_metrics(val_metrics, prefix='val')\n                self.model.train()  # Return to training mode\n            \n            # Save checkpoint\n            if (self.global_step % self.save_steps == 0 and \n                self.global_step > 0):\n                self._save_checkpoint(f\"step-{self.global_step}\")\n        \n        return total_loss / num_batches\n    \n    def _train_step(self, batch) -> float:\n        \"\"\"Execute one training step.\"\"\"\n        # Move batch to device\n        batch = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v \n                for k, v in batch.items()}\n        \n        # Forward pass with mixed precision\n        if self.use_mixed_precision:\n            with torch.cuda.amp.autocast():\n                outputs = self.model(**batch)\n                loss = outputs.loss / self.gradient_accumulation_steps\n        else:\n            outputs = self.model(**batch)\n            loss = outputs.loss / self.gradient_accumulation_steps\n        \n        # Backward pass\n        if self.use_mixed_precision:\n            self.scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        \n        # Gradient accumulation\n        if (self.global_step + 1) % self.gradient_accumulation_steps == 0:\n            # Gradient clipping\n            if self.use_mixed_precision:\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n                self.optimizer.step()\n            \n            # Update learning rate\n            if self.scheduler is not None:\n                self.scheduler.step()\n            \n            # Zero gradients\n            self.optimizer.zero_grad()\n        \n        self.global_step += 1\n        return loss.item() * self.gradient_accumulation_steps\n    \n    def _validate(self) -> Dict[str, float]:\n        \"\"\"Run validation.\"\"\"\n        self.model.eval()\n        total_loss = 0\n        num_batches = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(self.val_dataloader, desc=\"Validating\"):\n                batch = {k: v.to(self.model.device) if isinstance(v, torch.Tensor) else v \n                        for k, v in batch.items()}\n                \n                if self.use_mixed_precision:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(**batch)\n                else:\n                    outputs = self.model(**batch)\n                \n                total_loss += outputs.loss.item()\n                num_batches += 1\n        \n        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n        perplexity = torch.exp(torch.tensor(avg_loss)).item()\n        \n        return {\n            'val_loss': avg_loss,\n            'val_perplexity': perplexity\n        }\n    \n    def _should_stop_early(self, val_metrics: Dict[str, float]) -> bool:\n        \"\"\"Check if early stopping should be triggered.\"\"\"\n        if self.early_stopping_patience is None:\n            return False\n        \n        metric_name = self.config.get('metric_for_best_model', 'val_loss')\n        current_metric = val_metrics.get(metric_name, float('inf'))\n        \n        improved = False\n        if metric_name.endswith('loss'):\n            if current_metric < self.best_metric:\n                self.best_metric = current_metric\n                improved = True\n        else:\n            if current_metric > self.best_metric:\n                self.best_metric = current_metric\n                improved = True\n        \n        if improved:\n            self.early_stopping_counter = 0\n            self._save_model(\"best\")\n        else:\n            self.early_stopping_counter += 1\n        \n        return self.early_stopping_counter >= self.early_stopping_patience\n    \n    def _log_metrics(self, metrics: Dict[str, float], prefix: str = ''):\n        \"\"\"Log metrics to console and wandb.\"\"\"\n        if prefix:\n            metrics = {f\"{prefix}_{k}\": v for k, v in metrics.items()}\n        \n        # Add to training history\n        metrics['global_step'] = self.global_step\n        self.training_history.append(metrics.copy())\n        \n        # Log to wandb\n        if self.config.get('use_wandb', False):\n            wandb.log(metrics, step=self.global_step)\n    \n    def _save_checkpoint(self, checkpoint_name: str):\n        \"\"\"Save training checkpoint.\"\"\"\n        checkpoint_dir = self.output_dir / checkpoint_name\n        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Save model and tokenizer\n        self.model.save_pretrained(checkpoint_dir)\n        \n        # Save training state\n        training_state = {\n            'global_step': self.global_step,\n            'epoch': self.epoch,\n            'best_metric': self.best_metric,\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'config': self.config,\n            'training_history': self.training_history\n        }\n        \n        if self.scheduler is not None:\n            training_state['scheduler_state_dict'] = self.scheduler.state_dict()\n        \n        if self.use_mixed_precision:\n            training_state['scaler_state_dict'] = self.scaler.state_dict()\n        \n        torch.save(training_state, checkpoint_dir / \"training_state.pt\")\n        \n        print(f\"ðŸ’¾ Checkpoint saved: {checkpoint_dir}\")\n    \n    def _save_model(self, name: str):\n        \"\"\"Save the final model.\"\"\"\n        model_dir = self.output_dir / name\n        model_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model.save_pretrained(model_dir)\n        \n        # Save training history\n        with open(model_dir / \"training_history.json\", 'w') as f:\n            json.dump(self.training_history, f, indent=2)\n        \n        print(f\"ðŸ’¾ Model saved: {model_dir}\")\n    \n    def load_checkpoint(self, checkpoint_path: str):\n        \"\"\"Load a training checkpoint.\"\"\"\n        checkpoint_dir = Path(checkpoint_path)\n        training_state_path = checkpoint_dir / \"training_state.pt\"\n        \n        if not training_state_path.exists():\n            raise FileNotFoundError(f\"Training state not found: {training_state_path}\")\n        \n        training_state = torch.load(training_state_path, map_location=self.model.device)\n        \n        # Restore training state\n        self.global_step = training_state['global_step']\n        self.epoch = training_state['epoch']\n        self.best_metric = training_state['best_metric']\n        self.training_history = training_state.get('training_history', [])\n        \n        # Restore optimizer and scheduler\n        self.optimizer.load_state_dict(training_state['optimizer_state_dict'])\n        \n        if 'scheduler_state_dict' in training_state and self.scheduler is not None:\n            self.scheduler.load_state_dict(training_state['scheduler_state_dict'])\n        \n        if 'scaler_state_dict' in training_state and self.use_mixed_precision:\n            self.scaler.load_state_dict(training_state['scaler_state_dict'])\n        \n        print(f\"ðŸ”„ Checkpoint loaded: {checkpoint_path}\")\n        print(f\"   Global step: {self.global_step}\")\n        print(f\"   Epoch: {self.epoch}\")\n        print(f\"   Best metric: {self.best_metric}\")\n\n# Usage example\nif __name__ == \"__main__\":\n    # This would be integrated with your LlamaFarm setup\n    config = {\n        'epochs': 3,\n        'learning_rate': 2e-5,\n        'gradient_accumulation_steps': 4,\n        'fp16': True,\n        'early_stopping_patience': 3,\n        'use_wandb': True,\n        'wandb_project': 'llamafarm-advanced-training'\n    }\n    \n    # trainer = AdvancedTrainer(model, train_dataloader, val_dataloader, config=config)\n    # training_history = trainer.train()\n```"}